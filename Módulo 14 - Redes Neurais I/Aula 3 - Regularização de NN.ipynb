{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3 Regularização e Problemas do Gradiente Descendente\n",
    "\n",
    "## Temas:\n",
    "- Problemas do gradiente (vanish e exploding)\n",
    "- Underfit e Overfit\n",
    "- Regularização de Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema do vanishing-gradient\n",
    "\n",
    "O problema do gradiente de fuga (vanishing gradient) é encontrado ao __treinar redes neurais profundas__, com métodos de aprendizado baseados em gradiente e backpropagation. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de custo em relação ao peso atual. O problema é que, em alguns casos, __o gradiente será muito pequeno, impedindo efetivamente que o peso altere seu valor__. Na pior das hipóteses, isso __pode impedir completamente o treinamento__ da rede neural. \n",
    "\n",
    "Por exemplo, a função tangente hiperbólica tem gradientes no intervalo -1 a 1 e a sigmóide de 0 à 1 e como o backpropagation calcula gradientes pela regra da cadeia isso tem o efeito de multiplicar n desses pequenos números para calcular gradientes das camadas iniciais em uma rede de n camadas, o que significa que o gradiente diminui exponencialmente enquanto as camadas iniciais treinam muito lentamente.\n",
    "\n",
    "O resultado é um aprendizado lento, especialmente das primeiras camadas da rede.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "\n",
    "<center>\n",
    "<img src=https://i0.wp.com/neptune.ai/wp-content/uploads/Vanishing-and-Exploding-Gradients-in-Neural-Network-Models-Debugging-Monitoring-and-Fixing-Practical-Guide_7.png width=450 text=\"https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing\">\n",
    "\n",
    "\n",
    "<img src=https://i.stack.imgur.com/lVKg7.png text=https://stats.stackexchange.com/questions/432300/help-understanding-vanishing-and-exploding-gradients width=600>\n",
    "\n",
    "</center>\n",
    "\n",
    "$$w_j^{(i)} \\larr w_j^{(i)} - \\alpha \\frac{∂l}{∂w^{(i)}_{j}}$$\n",
    "\n",
    "$$ \\frac{∂l}{∂w^{(1)}_{1,1}}=\\frac{∂l}{∂o}⋅\\frac{∂o}{∂a^{(2)}_1}⋅\\frac{∂a^{(2)}_1}{∂a^{(1)}_1}⋅\\frac{∂a^{(1)}_1}{∂w^{(1)}_{1,1}}+\\frac{∂l}{∂o}⋅\\frac{∂o}{∂a^{(2)}_2}⋅\\frac{∂a^{(2)}_2}{∂a^{(1)}_1}⋅\\frac{∂a^{(1)}_1}{∂w^{(1)}_{1,1}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que o gradiente fosse 0.9 em uma rede com 100 camadas\n",
    "# ao multiplicar esse valor 100x temos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# novo peso = peso anterior - learning rate * gradiente\n",
    "novo_peso = \n",
    "novo_peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o gradiente é muito pequeno, ele influenciará pouco na atualização dos pesos, impactando no tempo de aprendizagem.\n",
    "\n",
    "Como identificar vanishing gradient\n",
    "- os parâmetros das últimas camadas sobrem mudanças grandes enquanto as primeiras quase não mudam\n",
    "- o modelo demora para aprender e o treino para após poucas iterações \n",
    "- performance do modelo é ruim\n",
    "\n",
    "\n",
    "Soluções:\n",
    "\n",
    "- Diminuir o tamanho da rede neural\n",
    "- Utilizar outras funções de ativação como a ReLU\n",
    "- Long short-term memory Networks (LSTM)\n",
    "- Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de análise que podemos fazer para avaliar o vanishing gradient\n",
    "\n",
    "<center>\n",
    "<img src=https://machinelearningmastery.com/wp-content/uploads/2021/11/vanishing-plot-sigmoid.png width=500 text=\"https://machinelearningmastery.com/visualizing-the-vanishing-gradient-problem/\">\n",
    "</center>\n",
    "\n",
    "No gráfico temos os dados dos pesos de cada uma das 5 camadas (cores) com função de ativação sigmóide realizadas no treinamento de 100 epochs (eixo x). No primeiro temos a média dos pesos, no segundo o desvio padrão e no terceiro o Loss ao longo das 100 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema do exploding-gradient\n",
    "\n",
    "O oposto pode acontecer quando a função de ativação não possui um limite máximo. A multiplicação devido ao gradiente ao longo da rede neural pode fazer os valores explodirem. No pior dos casos os pesos acabam recebendo valores NaN e não conseguem mais ser atualizados.\n",
    "\n",
    "O resultado é um modelo instável (em cada update a função perda varia muito) com grandes alterações a cada atualização.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*_YRWJr-jF7tKnmUq-e3ltw.png width=500 text=\"https://medium.com/@ayushch612/vanishing-gradient-and-exploding-gradient-problems-7737c0aa535f\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# novo peso = peso anterior - learning rate * gradiente\n",
    "novo_peso = \n",
    "novo_peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como identificar um gradient exploding:\n",
    "\n",
    "- a atualização dos pesos fica muito instável entre cada treino de batch/iteração\n",
    "- os parâmetros das primeiras camadas sobrem mudanças grandes enquanto das últimas mudam bem menos\n",
    "- pode haver o aparecimento de parâmetros nans\n",
    "- a loss function também pode ser nan\n",
    "\n",
    "Soluções:\n",
    "\n",
    "- Diminuir o tamanho da rede neural\n",
    "- Setar um valor máximo para o gradiente (Gradient clipping)\n",
    "- Verificar o tamanho dos pesos da rede e aplicar uma penalidade à função de perda (loss function) para valores de peso ($W$) grandes. Isso pode ser feito com as famosas regularização L1 (pesos absolutos) ou L2 (pesos quadrados).\n",
    "Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit e Overfit\n",
    "\n",
    "<center>\n",
    "<img src=https://i.pinimg.com/564x/72/e2/22/72e222c1542539754df1d914cb671bd7.jpg width=500>\n",
    "</center>\n",
    "\n",
    "Já sabemos como aumentar a complexidade da nossa rede neural e sair de um problema de underfiting. Mas o que fazer quando estamos overfittando nosso modelo?\n",
    "\n",
    "\n",
    "## Regularização de Redes Neurais\n",
    "\n",
    "### Regularização L1 e L2\n",
    "\n",
    "Assim como nos modelos clássicos de ML, aqui podemos adicionar um regularizador de pesos L1 ou L2 ao calcular a função perda. \n",
    "\n",
    "[Regularizadores disponíveis no Keras](https://keras.io/api/layers/regularizers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "O Dropout consiste em desativar, ou dropar, alguns neurônios randomicamente durante o treino. Isso evita que a rede torne-se muito dependente de alguns únicos neurônios, já que a rede neural não pode contar com ele todo o tempo. Isso torna o aprendizado da rede muito mais balanceado e reduz o overfiting.\n",
    "\n",
    "O *dropout_rate* indica quantos % dos neurônios daquela camada serão desligados aleatoriamente a cada conjunto de treino (batch) e seu valor, geralmente, fica entre 0.2 e 0.5.\n",
    "\n",
    "Atenção: o drop de neurônios só acontece no treino. No teste e validação não!\n",
    "\n",
    "<img src=https://www.i2tutorials.com/wp-content/media/2019/09/Deep-learning-41-i2tutorials.png width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte dos dados: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nas primeiras epochs o erro da validação está decrescendo, mas logo começa a subir rapidamente, enquanto o loss do treino continua em queda. Nesse momento começamos a ter overfit.\n",
    "\n",
    "É possível ver que o loss do treino quase chega a zero em 20 epochs.\n",
    "\n",
    "Vamos agora adicionar as regularizações e comparar seus efeitos no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar um modelo só com dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping (“Parada Antecipada” ou “Parada Precoce”)\n",
    "\n",
    "Como sabemos, o overfit acontece quando o erro no conjunto de validação começa a aumentar enquanto no treino continua a diminuir. O que o Early Stopping faz é observar após cada época como está a performance do modelo no conjunto de validação e se ela deixa de diminuir o modelo para de ser treinado.\n",
    "\n",
    "Para evitar de parar o modelo muito cedo, o que geralmente fazemos é observar se a performance do modelo não diminui por algum tempo.\n",
    "\n",
    "<img src=https://miro.medium.com/max/875/1*iAK5uMoOlX1gZu-cSh1nZw.png width=400>\n",
    "\n",
    "Vamos ver como utilizar o Early Stopping no modelo com dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplificar o modelo\n",
    "\n",
    "Assim como nos modelos clássicos, modelos mais complexos, ou seja, com mais parâmetros a serem treinados, tem mais chance de overfitar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização do batch\n",
    "\n",
    "A normalização em lote é um recurso que adicionamos entre as camadas da rede neural para normalizá-la usando média=0, padrão dev=1 (μ=0,σ=1) antes de enviá-la para a próxima camada. Essa normalização tem o efeito de estabilizar a rede neural e manter a distribuição dos dados, evitando valores muito grandes e muitos pequenos.\n",
    "\n",
    "Outro efeito da normalização do batch é o treinamento mais rápido, já que agora é possível utilizarmos valores mais altos para o learning rate, além de aumentar a precisão do modelo.\n",
    "\n",
    "<img src=https://149695847.v2.pressablecdn.com/wp-content/uploads/2020/07/batch_normalization.png width=500>\n",
    "\n",
    "\n",
    "https://analyticsindiamag.com/hands-on-guide-to-implement-batch-normalization-in-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentar o tamanho da base de treino\n",
    "Isso pode ser feito adquirindo novos dados ou criando novos dados artificialmente (data augmentation).\n",
    "\n",
    "<img src=https://amitness.com/images/nlp-aug-bert-augmentations.png width=500>\n",
    "\n",
    "Exemplos de data augmentation em NLP: https://www.analyticsvidhya.com/blog/2022/02/text-data-augmentation-in-natural-language-processing-with-texattack/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "\n",
    "https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\n",
    "\n",
    "https://analyticsindiamag.com/hands-on-guide-to-implement-batch-normalization-in-deep-learning-models/\n",
    "\n",
    "[Exemplos de data augmentation em NLP](https://www.analyticsvidhya.com/blog/2022/02/text-data-augmentation-in-natural-language-processing-with-texattack/)\n",
    "\n",
    "[Vanishing Gradient explained using Code!](https://www.youtube.com/watch?v=wTyZqtJyp5g)\n",
    "\n",
    "[Investigando o problema de vanishing gradient](https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício:\n",
    "\n",
    "Retreine o modelo do exercício da aula anterior adicionando algumas regularizações vistas nessa aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb5f626699f206ef97176a4f092b8d9f6e52ae1f84b4bb3163daf9eb25ca3519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
