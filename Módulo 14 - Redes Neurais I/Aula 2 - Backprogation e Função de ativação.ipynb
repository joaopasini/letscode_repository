{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes Neurais Artificiais**\n",
    "\n",
    "## **Reprise**\n",
    "\n",
    "<img src=https://iq.opengenus.org/content/images/2021/11/step-func-1-2.png width=500 text=\"https://iq.opengenus.org/binary-step-function/\" >\n",
    "\n",
    "#### E como a RNA aprende os pesos? Quero entender um pouco melhor estes modelos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-win_amd64.whl (455.9 MB)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from pydot) (2.4.7)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.49.1-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.5-cp38-cp38-win_amd64.whl (896 kB)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.11.1-py2.py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\joao\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow, pydot, graphviz\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-2.0.7 gast-0.4.0 google-auth-2.11.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 graphviz-0.20.1 grpcio-1.49.1 importlib-metadata-4.12.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.1 opt-einsum-3.3.0 protobuf-3.19.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow graphviz pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forward Propagation x Backward Propagation**\n",
    "\n",
    "As redes neurais tem como objetivo realizar o aprendizado de pesos que melhor se ajustam aos dados de entrada e essa \"aprendizagem\" é realizada através de duas fases:\n",
    "\n",
    "- **Forward Propagation**;\n",
    "- **Backward Propagation**.\n",
    "\n",
    "No __forward propagation__, a informação propaga na direção habitual (da camada de input para a de output) na rede neural: features são lidas na camada de input, passam pelo processamento nas camadas ocultas, e a resposta (target) é predita na camada de output. \n",
    "\n",
    "Para que a predição seja realizada, os neurônios nas camadas ocultas e de output realizam as seguintes duas etapas de cálculo:\n",
    "\n",
    "- 1. Uma combinação linear entre o output da camada anterior (que denotamos pela letra **a**) e os pesos da camada atual (denominados de **W**). Assim, se tivermos n ligações a combinação linear que teremos será:\n",
    "\n",
    "<center>\n",
    "<img src=https://latex.codecogs.com/gif.latex?z%5E%7Batual%7D%20%3D%20W_0%5E%7Batual%7D%20&plus;%20%5Cleft%20%28%20W_1%5E%7Batual%7D%5Ctimes%20a_1%5E%7Banterior%7D%20%5Cright%29%20&plus;%20%5Cleft%28%20W_2%5E%7Batual%7D%20%5Ctimes%20a_2%5E%7Banterior%7D%20%5Cright%20%29%20&plus;%20%5Ccdots%20&plus;%20%5Cleft%20%28%20W_n%5E%7Batual%7D%20%5Ctimes%20a_n%5E%7Banterior%7D%20%5Cright%29) width=700>\n",
    "</center>\n",
    "\n",
    "- 2. Aplica-se uma **função de ativação** $f(x)$ não-linear à combinação linear acima. \n",
    "\n",
    "As principais funções de ativação utilizadas são:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\n",
    "\" width=\"600\" />\n",
    "</center>\n",
    "\n",
    "O cálculo realizado por um único neurônio é bem parecido com um **perceptron**, ilustrado a seguir:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://img2.gratispng.com/20180619/oav/kisspng-multilayer-perceptron-machine-learning-statistical-5b2996bdb9dcd2.4724873615294522217613.jpg\" width=\"400\" />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notação do feedforward\n",
    "\n",
    "Soma dos pesos com os valores da camada anterior é denominado de $z^{atual}$\n",
    "\n",
    "$$z^{atual} = W_n^{atual} \\ast x_n^{anterior}   + b^{atual} = \\sum_n{W_n^{atual} x_n^{anterior}}   + b^{atual}$$\n",
    "\n",
    "Aplica função de ativação $f$ para obter $a^{atual}$\n",
    "\n",
    "$$a^{atual} = f(z^{atual})$$\n",
    "\n",
    "Na camada de output teremos que:\n",
    "\n",
    "$$\\hat{y} = a^{atual}$$\n",
    "\n",
    "E podemos calcular nossa função de custo (vamos aqui considerar o MSE, por exemplo):\n",
    "\n",
    "$$C = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Sandra-Vieira-7/publication/312205163/figure/fig1/AS:453658144972800@1485171938968/a-The-building-block-of-deep-neural-networks-artificial-neuron-or-node-Each-input-x.png\" width=\"400\" text=\"https://www.researchgate.net/publication/312205163_Using_deep_learning_to_investigate_the_neuroimaging_correlates_of_psychiatric_and_neurological_disorders_Methods_and_applications\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"forward_pass.png\" width=300>\n",
    "</center>\n",
    "\n",
    "[Link](https://ml4a.github.io/demos/simple_forward_pass/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual função de ativação devo utilizar?\n",
    "\n",
    "A escolha das funções de ativação também pode ser variável, mas costuma-se utilizar:\n",
    "\n",
    "**Camada de output**:\n",
    "- **Sigmoid** (para problemas de classificação binários) ou **Softmax** (para problemas de classificação multiclasse)\n",
    "- **Linear** para problemas de regressão \n",
    "\n",
    "Quando a __função de ativação é utilizada em camadas de saída, o objetivo é estruturar aquela saída de acordo com a saída do nosso problema__. Nesse contexto, a função softmax estrutura a saída de forma a poder realizar uma classificação multiclasse, da mesma forma que a função sigmoid estrutura a saída de um problema binário.\n",
    "\n",
    "<img src=https://machinelearningmastery.com/wp-content/uploads/2020/12/How-to-Choose-an-Output-Layer-Activation-Function.png width=500 text=\"https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\">\n",
    "\n",
    "**Camadas ocultas**\n",
    "\n",
    "Essas __funções de ativação__, quando utilizadas nas camadas ocultas, permitem que __adicionemos não linearidade__ na resolução da nossa solução. Se observamos a figura do perceptron ilustrada acima, teremos sempre uma equação linear para representação do nosso problema, já que teremos basicamente uma soma ponderada. Entretanto, muitas vezes nossa base de dados requer uma equação não linear para representar o problema. A tendência é que, quanto mais difícil o problema seja, mais chances da resolução ser não linear.\n",
    "\n",
    "Sendo assim, um processo comum é, após a soma ponderada, gerar uma saída não linear por meio das funções de ativação.\n",
    "\n",
    "Já a __ReLU (Rectified Linear Unit) tem o papel de jogar para zero qualquer valor negativo, gerando uma matriz muito mais esparsa__. Com isso teremos uma matriz com vários zeros e alguns não zeros o que __força a rede a focar nestes últimos tornando muito mais eficiente o tempo computacional de treino__. Isso também contruibui com a __diminuição do \"vanishing gradient problem\"__, que discutiremos nas próximas aulas, o que permite que o modelo aprenda mais rápido e performe melhor.\n",
    "\n",
    "<img src=https://machinelearningmastery.com/wp-content/uploads/2020/12/How-to-Choose-an-Hidden-Layer-Activation-Function.png width=500 text=\"https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\">\n",
    "\n",
    "\n",
    "\n",
    "Ao fim do forward propagation, na camada de output, calculamos a **função de perda**, que quantifica qual a **diferença entre as predições feitas pela rede neural e os valores reais do target dos dados**. Cada tipo de problema tem uma função de perda própria.\n",
    "\n",
    "Queremos que as predições sejam sempre o mais próximas o possível dos valores reais!\n",
    "\n",
    "Isto é feito ao propagarmos a informação na direção contrária (de trás pra frente) na rede neural, o que caracteriza o chamado __backward propagation__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Backward propagation__\n",
    "<img src=https://cdn-images-1.medium.com/max/540/1*7CJRMAomDwc8HBtaD6LqlQ.gif text=\"https://www.louisbouchard.ai/densenet-explained/\" width=500>\n",
    "\n",
    "Nesse processo de __backward propagation__ os pesos $W_i$ são atualizados de acordo com sua contribuição no erro final e esse atualização acontece de trás para frente na rede. Uma vez que o erro propagado chega na primeira camada outra amostra é iniciada alimentando a rede neural. \n",
    "\n",
    "Este processo de forward e backward propagation é feito iterativamente, várias vezes utilizando todo o dataset. Cada vez que o dataset inteiro passa por esse processo chamamos de **epoch**.\n",
    "\n",
    "O objetivo do backward propagation é **determinar os pesos que miminizem a função de perda!** A cada iteração, os pesos são **atualizados**, de modo que a função de perda é sempre reduzida em direção ao seu mínimo.\n",
    "\n",
    "<img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como esses pesos são atualizados?\n",
    "\n",
    "A forma como esses pesos são atualizados para reduzir a função de perda depende do **otimizador** escolhido. Os otimizadores são objetos que representam o procedimento matemático de minimização da função de perda.\n",
    "\n",
    "Os principais otimizadores utilizados são: \n",
    "\n",
    "- __Gradiente Descendente (GD)__,\n",
    "- **Stochastic Gradient Descent (SGD)**,\n",
    "- **Adam** \n",
    "- **RMSProp** \n",
    "\n",
    "(vale a pena testar cada um deles!)\n",
    "\n",
    "### Gradient Descent\n",
    "Você começa definindo os valores do parâmetro inicial e, a partir daí, o gradiente descendente usa o cálculo para ajustar iterativamente os valores para que eles minimizem a função de custo fornecida em cada uma das epochs.\n",
    "\n",
    "<img src=https://static.wixstatic.com/media/3eee0b_4aff9459e3ad43ae9b9e18b2c5631fc1~mv2.jpg/v1/fill/w_360,h_178,al_c,lg_1,q_90/3eee0b_4aff9459e3ad43ae9b9e18b2c5631fc1~mv2.webp width=300 text=\"https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\">\n",
    "\n",
    "Nessa equação $\\theta$ representa nossos pesos $W$ e $J(\\theta)$ é nossa função custo (MSE, por exemplo). Lembrando que a função custo é uma fórmula matemática que permite que nosso modelo de ML analise o quão bem ele fitou nos dados.\n",
    "\n",
    "Assim, podemos concluir dessa equação que a atualização dos parâmetro $\\theta$ depende do seu valor anterior, qual a taxa de variação a função de custo $J(\\theta)$ devido ao $\\theta$ e do learning rate $\\alpha$. Nessa estratégia o objetivo é chegarmos no valor mínimo dá nossa função de custo avaliando o gradiente (derivada) da nossa função:\n",
    "\n",
    "<img src=https://static.wixstatic.com/media/3eee0b_ed42ef8479934026980c15c679df0821~mv2.png/v1/fill/w_360,h_224,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_ed42ef8479934026980c15c679df0821~mv2.webp width=400 text=\"https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\">\n",
    "\n",
    "O learning rate $\\alpha$ é o valor responsável por definir qual o tamanho do passo de cada iteração. Se considerarmos um valor muito grande para o learning rate, podemos nunca chegar no valor de mínimo enquanto se utilizarmos um valor muito baixo podemos demorar muito para chegar.\n",
    "\n",
    "<img src=https://static.wixstatic.com/media/3eee0b_08ae4ed9e6504acbb3bd37320c20f77e~mv2.png/v1/fill/w_360,h_187,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_08ae4ed9e6504acbb3bd37320c20f77e~mv2.webp width=400 text=\"https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\">\n",
    "\n",
    "Para saber como está performando nosso learning rate podemos plotar seu valor versus quanto o erro está variando.\n",
    "\n",
    "<img src=https://806230.smushcdn.com/1739487/wp-content/uploads/2020/10/plot-min.png text=\"https://sdsclub.com/stochastic-gradient-descent-vs-gradient-descent-a-head-to-head-comparison/\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hora do gif\n",
    "\n",
    "##### Gradiente descentente unidimensional\n",
    "\n",
    "A curva representada por $C(w)$ corresponde a função de custo que queremos minimizar.\n",
    "\n",
    "<img src=https://mlfromscratch.com/content/images/2019/12/gradient-descent-optimized.gif width=600 text=\"https://mlfromscratch.com/neural-networks-explained/\">\n",
    "\n",
    "Se o gradiente das derivadas parciais for positivo damos um passo à esquerda, e damos um passo à direita quando negativo.\n",
    "\n",
    "##### Gradiente descentente bidimensional\n",
    "\n",
    "A superfície representada corresponde a função de custo que queremos minimizar.\n",
    "\n",
    "<img src=https://miro.medium.com/max/598/1*hUd744hDEEGx0-ypWGhrkw.gif width=300 text='https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c'>\n",
    "\n",
    "__Vantagens:__\n",
    "\n",
    "- Computação fácil.\n",
    "- Fácil de implementar.\n",
    "- Fácil de entender.\n",
    "\n",
    "__Desvantagens:__\n",
    "\n",
    "- Pode prender em mínimos locais.\n",
    "- Os pesos são alterados após o cálculo do gradiente em todo o conjunto de dados. Portanto, se o conjunto de dados for muito grande, isso pode levar anos para convergir para os mínimos.\n",
    "- Requer grande memória para calcular o gradiente em todo o conjunto de dados.\n",
    "\n",
    "Para quem quiser saber mais sobre o gradiente descendente e sobre derivadas eu recomendo o capítulo \"2.4 The engine of neural networks: Gradient-based\n",
    "optimization\" do livro \"Deep Learning with Python, 2nd Edition\" do Françõis Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "No SGD a derivada é computada considerando apenas um ponto por vez, o que evita o problema de memória.\n",
    "\n",
    "<img src=https://static.wixstatic.com/media/3eee0b_2f20c4c9902844718350e189e57fd909~mv2.png/v1/fill/w_740,h_290,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_2f20c4c9902844718350e189e57fd909~mv2.webp width=500 text=\"https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\">\n",
    "\n",
    "No SGD em cada passo o gradiente descendente é cálculado para cada amostra enquanto no GD ele é cálculado para todo o dataset. Isso significa que os pesos são atualizados no GD apenas após todo o dataset ter passado pela rede e termos os valores de $\\hat{y}$ para cada amostra. No SGD, os pesos são atualizados toda vez que uma amostra chega ao final da rede neural.\n",
    "\n",
    "Então após 1 epoch o GD teria calculado apenas um gradiente descendente enquando o SGD teria calculado `X_train.shape[0]` vezes.\n",
    "\n",
    "__Vantagem:__\n",
    "\n",
    "- O requisito de memória é menor em comparação com o algoritmo GD, pois a derivada é calculada tomando apenas 1 ponto de cada vez.\n",
    "- Minimiza a função de custo de forma mais rápida que o GD (por ter mais atualizações dos pesos em 1 epoch)\n",
    "\n",
    "__Desvantagens:__\n",
    "\n",
    "- O tempo necessário para completar 1 época é grande comparado ao algoritmo GD.\n",
    "- Ainda leva muito tempo para convergir.\n",
    "- Também pode ficar preso em mínimos locais.\n",
    "- SGD oscilar mais e necessita de muito mais passos\n",
    "\n",
    "\n",
    "### Mini-bath gradient descent (MGD)\n",
    "Para conseguir o melhor dos dois podemos utilizar o Mini-bath gradient descent (MGD) que considera uma pequena amostra de dataset de treino em cada iteração. Esse otimizador é mais estável que o SGD.\n",
    "\n",
    "O MGD divide o conjunto de dados em vários lotes e após cada lote os parâmetros são atualizados.\n",
    "\n",
    "<img src=https://static.wixstatic.com/media/3eee0b_afe86f0d655d4b218f002ce82c1c25ac~mv2.png/v1/fill/w_740,h_190,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_afe86f0d655d4b218f002ce82c1c25ac~mv2.webp width=500 text=\"https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\">\n",
    "\n",
    "__Vantagens:__\n",
    "\n",
    "- Menor complexidade de tempo para convergir em comparação com o algoritmo SGD padrão.\n",
    "\n",
    "__Desvantagens:__\n",
    "\n",
    "- A atualização do MB-SGD é muito ruidosa em comparação com a atualização do algoritmo GD.\n",
    "-  Leva mais tempo para convergir do que o algoritmo GD.\n",
    "-  Pode ficar preso em mínimos locais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD x SGD x MGD\n",
    "\n",
    "__Gradient Descent__ -> Batch Size = Size of Training Set\n",
    "\n",
    "__Stochastic Gradient Descent__ -> Batch Size = 1\n",
    " \n",
    "__Mini-Batch Gradient Descent__ -> 1 < Batch Size < Size of Training Set\n",
    "\n",
    "E se o conjunto de dados não for dividido uniformemente pelo tamanho do lote?\n",
    "\n",
    "Isso acontece com frequência ao treinar um modelo. Significa simplesmente que o lote final tem menos amostras do que os outros lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "Como vimos nos outros otimizadores, a escolha do learning rate é fundamental tanto para a convergência quanto para o tempo de convergência. Para minimizar esse problema, o RMSProp utiliza um **decaimento exponencial do learning rate**. Dessa forma, o learning rate é alto no início da aprendizagem e seu valor cai ao longo do tempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Moment Estimation (Adam)\n",
    "Adam pode ser considerado uma combinação de Stochastic Gradient Descent e RMSProp com momento.\n",
    "\n",
    "#### Momento?\n",
    "\n",
    "O conceito de momento aqui é o mesmo que utilizamos em física. Suponha uma bola rolando em uma superfície sem atrito. O momento faz com que ela continue seu movimento sem parar.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/400/1*i1Qc2E0TVlPHEKG7LepXgA.gif width=500>\n",
    "</center>\n",
    "\n",
    "<figcaption align = \"center\"><b>Fig.1 - Descida do momento com decay_rate = 1.0 (sem decaimento).</b></figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem momento:\n",
    "\n",
    "$$ \\theta = \\theta - learning\\_rate * gradient$$\n",
    "\n",
    "Com momento:\n",
    "\n",
    "$$\\theta = \\theta - learning\\_rate * gradient + previous\\_theta * decay\\_rate$$\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/875/1*pgpFmmIXJBvlReVwlRQ-Yg.png width=700>\n",
    "</center>\n",
    "\n",
    "<figcaption align = \"center\"><b>Fig.2 - Como o momento atua passo-a-passo.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "Normalmente, a taxa de decaimento (decay_rate) é escolhida em torno de 0,8 a 0,9 – é como uma superfície com um pouco de atrito, então, eventualmente, a bolinha desacelera e para.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/500/1*zVi4ayX9u0MQQwa90CnxVg.gif width=400>\n",
    "</center>\n",
    "\n",
    "<figcaption align = \"center\"><b>Fig.3 - Momentum (magenta) vs. Gradient Descent (ciano) em uma superfície com um mínimo global (o poço esquerdo) e um mínimo local (o poço direito).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "Qual a diferença entre GD com e sem momentum:\n",
    "\n",
    "- Momentum se move mais rápido por causa de todo o momentum que acumula\n",
    "- O momento tem uma chance de escapar de mínimos locais (porque o momento pode impulsioná-lo para fora de um mínimo local).\n",
    "- Percorrer melhor as regiões de planalto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Como construir um modelo de Rede Neural Artificial em Python?**\n",
    "\n",
    "Vamos iniciar reescrevendo nosso perceptron com a biblioteca `keras`. Para isso precisamos introduzir alguns conceitos novos e aproveitaremos para revisar outros:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!pesquisar como o perceptron do sklearn atualiza os pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "# Importa o dataset do iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    " \n",
    "# Separa em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Vamos fazer um scale dos nossos dados\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'class_weight': None,\n",
       " 'early_stopping': False,\n",
       " 'eta0': 1.0,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 5,\n",
       " 'n_jobs': None,\n",
       " 'penalty': None,\n",
       " 'random_state': 0,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = Perceptron()\n",
    "pp.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicionário de termos + Keras\n",
    "\n",
    "[Sequencial](https://keras.io/api/models/sequential/)\n",
    "\n",
    "O bloco de construção inicial do Keras é um modelo, e o modelo mais simples é chamado de _sequencial_. Um modelo Keras sequencial é um pipeline linear (uma pilha) de camadas de redes neurais. Outras formas de criar modelos são com o [Functional API](https://keras.io/guides/functional_api) e [Model Subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing)\n",
    "\n",
    "[Input layer](https://keras.io/api/layers/core_layers/input/)\n",
    "\n",
    "É a camada de input que contem nossas features. O dado de entrada deve ser um array de uma única coluna.\n",
    "\n",
    "[Dense layer](https://keras.io/api/layers/core_layers/dense/)\n",
    "\n",
    "Dense layers (camadas densas), são camadas neurais densamente conectadas. Isso significa que todos os neurônios de uma camada se conectam com todos os neurônios da camada seguinte. Essas camadas também são chamadas de fully connected).\n",
    "\n",
    "<img src=https://www.i2tutorials.com/wp-content/media/2019/09/Deep-learning-41-i2tutorials.png width=500>\n",
    "\n",
    "\n",
    "[Funções de Custo](https://keras.io/api/losses/)\n",
    "\n",
    "O objetivo das funções de perda é calcular a quantidade que um modelo deve procurar minimizar durante o treinamento.\n",
    "- CategoricalCrossentropy: many-class classification; target no formato one-hot-encode ([1,0,0] , [0,1,0], [0,0,1])\n",
    "- SparseCategoricalCrossentropy: many-class classification; target como coluna única ([1] , [2], [3])\n",
    "- BinaryCrossentropy: two-class classification\n",
    "- MeanSquaredError\n",
    "- KLDivergence\n",
    "- CosineSimilarity\n",
    "\n",
    "[Métricas](https://keras.io/api/metrics/)\n",
    "\n",
    "Uma métrica é uma função usada para avaliar o desempenho do seu modelo.\n",
    "\n",
    "As funções de métrica são semelhantes às funções de perda, exceto que os resultados da avaliação de uma métrica não são usados ​​ao treinar o modelo. Observe que você pode usar qualquer função de perda como métrica.\n",
    "\n",
    "- CategoricalCrossentropy: target é uma matrix com cada coluna representando uma classe\n",
    "- SparseCategoricalCrossentropy: target é uma única coluna com as classes sendo representadas de 0 até n.\n",
    "- BinaryCrossentropy: target binário\n",
    "- Accuracy\n",
    "- AUC\n",
    "- MeanSquaredError\n",
    "- KLDivergence\n",
    "- CosineSimilarity\n",
    "\n",
    "[Funções de ativação](https://keras.io/api/layers/activations)\n",
    "\n",
    "Função aplicada nas camadas ocultas e de output visando aumentar a complexidade do nosso modelo, no primeiro caso, e adequar nossa saída.\n",
    "\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/875/1*B24qtkjFBD43ClulZtLmVA.png width=600 text=\"https://medium.com/analytics-vidhya/activation-functions-and-loss-functions-for-neural-networks-how-to-pick-the-right-one-542e1dd523e0\">\n",
    "</center>\n",
    "\n",
    "[Inicializador de pesos](https://keras.io/api/layers/initializers)\n",
    "\n",
    "Como queremos iniciar os pesos $W$.\n",
    "\n",
    "- __random_uniform:__ os pesos são inicializados para valores pequenos uniformemente aleatórios em (-0,05, 0,05). Em outras palavras, qualquer valor dentro do intervalo dado tem a mesma probabilidade de ser desenhado.\n",
    "- __random_normal:__ Os pesos são inicializados de acordo com uma Gaussiana, com média zero e desvio padrão pequeno de 0,05. Para aqueles que não estão familiarizados com uma gaussiana, pense em uma forma de curva de sino simétrica.\n",
    "- __zero:__ Todos os pesos são inicializados com o valor zero.\n",
    "\n",
    "[Otimizador](https://keras.io/api/optimizers/)\n",
    "\n",
    "O mecanismo pelo qual o modelo se atualizará com base nos dados de treinamento que ele vê, para melhorar seu desempenho.\n",
    "\n",
    "- SGD (with or without momentum)\n",
    "- RMSprop\n",
    "- Adam\n",
    "- Adagrad\n",
    "\n",
    "[Epochs]()\n",
    "\n",
    "Quantas vezes o loop de treinamento deve iterar sobre todos os dados.\n",
    "\n",
    "Uma época significa que cada amostra no conjunto de dados de treinamento teve a oportunidade de atualizar os parâmetros internos do modelo. Uma época é composta por um ou mais lotes e geralmente assume valores bem grandes como 100, 500, 1000 e até mais. Ao final de cada epoch temos uma estimativa do erro do nosso modelo.\n",
    "\n",
    "[Batch]()\n",
    "\n",
    "O tamanho do lote a ser usado em cada época de descida do Mini-batch GD.\n",
    "O número de exemplos de treinamento considerados para calcular os gradientes para um etapa de atualização de peso.\n",
    "Normalmente entre 8 e 128 que são processados ​​simultaneamente pelo modelo. O número de amostras é\n",
    "geralmente uma potência de 2, para facilitar a alocação de memória na GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 2, 1, 2, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.1371 - accuracy: 0.3095 - val_loss: 1.0479 - val_accuracy: 0.4286\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.0865 - accuracy: 0.3095 - val_loss: 1.0389 - val_accuracy: 0.4286\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.0628 - accuracy: 0.3095 - val_loss: 1.0333 - val_accuracy: 0.4286\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.0421 - accuracy: 0.3214 - val_loss: 1.0274 - val_accuracy: 0.4286\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.0255 - accuracy: 0.3571 - val_loss: 1.0254 - val_accuracy: 0.2381\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.0092 - accuracy: 0.3571 - val_loss: 1.0237 - val_accuracy: 0.2381\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9948 - accuracy: 0.3571 - val_loss: 1.0162 - val_accuracy: 0.2381\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.9829 - accuracy: 0.3810 - val_loss: 1.0059 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.9695 - accuracy: 0.4405 - val_loss: 0.9915 - val_accuracy: 0.4762\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.9591 - accuracy: 0.6429 - val_loss: 0.9868 - val_accuracy: 0.5714\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9472 - accuracy: 0.6190 - val_loss: 0.9788 - val_accuracy: 0.5714\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9364 - accuracy: 0.6786 - val_loss: 0.9704 - val_accuracy: 0.5714\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9240 - accuracy: 0.6905 - val_loss: 0.9551 - val_accuracy: 0.5714\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9129 - accuracy: 0.6905 - val_loss: 0.9471 - val_accuracy: 0.5714\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.9018 - accuracy: 0.6905 - val_loss: 0.9373 - val_accuracy: 0.5714\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8921 - accuracy: 0.6905 - val_loss: 0.9317 - val_accuracy: 0.5714\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8809 - accuracy: 0.6905 - val_loss: 0.9221 - val_accuracy: 0.5714\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8717 - accuracy: 0.6905 - val_loss: 0.9153 - val_accuracy: 0.5714\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8611 - accuracy: 0.6905 - val_loss: 0.9044 - val_accuracy: 0.5714\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - accuracy: 0.6905 - val_loss: 0.8921 - val_accuracy: 0.5714\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8409 - accuracy: 0.6905 - val_loss: 0.8804 - val_accuracy: 0.5714\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8319 - accuracy: 0.6905 - val_loss: 0.8710 - val_accuracy: 0.5714\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8230 - accuracy: 0.6905 - val_loss: 0.8590 - val_accuracy: 0.5714\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8151 - accuracy: 0.6905 - val_loss: 0.8492 - val_accuracy: 0.5714\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8053 - accuracy: 0.6905 - val_loss: 0.8445 - val_accuracy: 0.5714\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7971 - accuracy: 0.6905 - val_loss: 0.8321 - val_accuracy: 0.5714\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7896 - accuracy: 0.6905 - val_loss: 0.8247 - val_accuracy: 0.5714\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7815 - accuracy: 0.6905 - val_loss: 0.8173 - val_accuracy: 0.5714\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7731 - accuracy: 0.6905 - val_loss: 0.8087 - val_accuracy: 0.5714\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7662 - accuracy: 0.6905 - val_loss: 0.7994 - val_accuracy: 0.5714\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7577 - accuracy: 0.6905 - val_loss: 0.7942 - val_accuracy: 0.5714\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7502 - accuracy: 0.6905 - val_loss: 0.7868 - val_accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7428 - accuracy: 0.6905 - val_loss: 0.7832 - val_accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7368 - accuracy: 0.6905 - val_loss: 0.7749 - val_accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7289 - accuracy: 0.6905 - val_loss: 0.7648 - val_accuracy: 0.5714\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7229 - accuracy: 0.6905 - val_loss: 0.7621 - val_accuracy: 0.5714\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7161 - accuracy: 0.6905 - val_loss: 0.7518 - val_accuracy: 0.5714\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7084 - accuracy: 0.6905 - val_loss: 0.7444 - val_accuracy: 0.5714\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7035 - accuracy: 0.6905 - val_loss: 0.7373 - val_accuracy: 0.5714\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.6905 - val_loss: 0.7360 - val_accuracy: 0.5714\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.6905 - val_loss: 0.7314 - val_accuracy: 0.5714\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.6905 - val_loss: 0.7219 - val_accuracy: 0.5714\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.6905 - val_loss: 0.7127 - val_accuracy: 0.5714\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.6905 - val_loss: 0.7099 - val_accuracy: 0.5714\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.6905 - val_loss: 0.7040 - val_accuracy: 0.5714\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6621 - accuracy: 0.6905 - val_loss: 0.6984 - val_accuracy: 0.5714\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.6905 - val_loss: 0.6936 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6526 - accuracy: 0.6905 - val_loss: 0.6870 - val_accuracy: 0.5714\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.6905 - val_loss: 0.6825 - val_accuracy: 0.5714\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.6905 - val_loss: 0.6769 - val_accuracy: 0.5714\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6339 - accuracy: 0.6905 - val_loss: 0.6748 - val_accuracy: 0.5714\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6905 - val_loss: 0.6687 - val_accuracy: 0.5714\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6240 - accuracy: 0.6905 - val_loss: 0.6651 - val_accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.6905 - val_loss: 0.6580 - val_accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6153 - accuracy: 0.6905 - val_loss: 0.6489 - val_accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.7143 - val_loss: 0.6482 - val_accuracy: 0.5714\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.6905 - val_loss: 0.6468 - val_accuracy: 0.5714\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6905 - val_loss: 0.6441 - val_accuracy: 0.5714\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.6905 - val_loss: 0.6358 - val_accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.6905 - val_loss: 0.6346 - val_accuracy: 0.5714\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5903 - accuracy: 0.6905 - val_loss: 0.6251 - val_accuracy: 0.6190\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.7024 - val_loss: 0.6215 - val_accuracy: 0.6190\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5837 - accuracy: 0.6905 - val_loss: 0.6205 - val_accuracy: 0.5714\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.6905 - val_loss: 0.6209 - val_accuracy: 0.5714\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5760 - accuracy: 0.6905 - val_loss: 0.6121 - val_accuracy: 0.6190\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.6905 - val_loss: 0.6065 - val_accuracy: 0.6190\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7024 - val_loss: 0.6019 - val_accuracy: 0.6190\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7024 - val_loss: 0.6020 - val_accuracy: 0.6190\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5611 - accuracy: 0.6905 - val_loss: 0.5972 - val_accuracy: 0.6190\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7143 - val_loss: 0.5996 - val_accuracy: 0.5714\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.6905 - val_loss: 0.5920 - val_accuracy: 0.6190\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5506 - accuracy: 0.6905 - val_loss: 0.5835 - val_accuracy: 0.6190\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7262 - val_loss: 0.5805 - val_accuracy: 0.6190\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5440 - accuracy: 0.7262 - val_loss: 0.5779 - val_accuracy: 0.6190\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7381 - val_loss: 0.5795 - val_accuracy: 0.6190\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.6905 - val_loss: 0.5749 - val_accuracy: 0.6190\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7143 - val_loss: 0.5688 - val_accuracy: 0.6190\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7143 - val_loss: 0.5647 - val_accuracy: 0.6190\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7619 - val_loss: 0.5666 - val_accuracy: 0.6190\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5275 - accuracy: 0.7143 - val_loss: 0.5629 - val_accuracy: 0.6190\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5236 - accuracy: 0.7381 - val_loss: 0.5624 - val_accuracy: 0.6190\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.7024 - val_loss: 0.5606 - val_accuracy: 0.6190\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5183 - accuracy: 0.7381 - val_loss: 0.5578 - val_accuracy: 0.6190\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7381 - val_loss: 0.5591 - val_accuracy: 0.6190\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5134 - accuracy: 0.6905 - val_loss: 0.5475 - val_accuracy: 0.6190\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.7619 - val_loss: 0.5481 - val_accuracy: 0.6190\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7024 - val_loss: 0.5444 - val_accuracy: 0.6190\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7381 - val_loss: 0.5383 - val_accuracy: 0.6190\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5044 - accuracy: 0.7619 - val_loss: 0.5372 - val_accuracy: 0.6190\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7500 - val_loss: 0.5347 - val_accuracy: 0.6190\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7500 - val_loss: 0.5325 - val_accuracy: 0.6190\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7500 - val_loss: 0.5294 - val_accuracy: 0.6667\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.8333 - val_loss: 0.5328 - val_accuracy: 0.6190\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7500 - val_loss: 0.5304 - val_accuracy: 0.6190\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4889 - accuracy: 0.7738 - val_loss: 0.5326 - val_accuracy: 0.6190\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7143 - val_loss: 0.5248 - val_accuracy: 0.6190\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7381 - val_loss: 0.5231 - val_accuracy: 0.6190\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7381 - val_loss: 0.5130 - val_accuracy: 0.7619\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7738 - val_loss: 0.5123 - val_accuracy: 0.7619\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7857 - val_loss: 0.5093 - val_accuracy: 0.7619\n"
     ]
    }
   ],
   "source": [
    "from tabnanny import verbose\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# declarar forma modelo\n",
    "model = Sequential(name='Perceptron')\n",
    "\n",
    "# adicionar os layers\n",
    "# para remover o último layer usamos o .pop()\n",
    "model.add(Input(X_train.shape[1]))\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer='random_uniform', name='output'))\n",
    "\n",
    "# compilar o modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit do modelo\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7778\n",
      "[0.49279701709747314, 0.7777777910232544]\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training\n",
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Perceptron\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " output (Dense)              (None, 3)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 neurônios de input * 3 neurônios de output + 3 bias\n",
    "4 * 3 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP no Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.0865 - accuracy: 0.3452 - val_loss: 1.0665 - val_accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.0471 - accuracy: 0.3810 - val_loss: 1.0522 - val_accuracy: 0.2381\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 1.0163 - accuracy: 0.3571 - val_loss: 1.0321 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9799 - accuracy: 0.4643 - val_loss: 0.9941 - val_accuracy: 0.5714\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.9318 - accuracy: 0.6905 - val_loss: 0.9460 - val_accuracy: 0.5714\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8797 - accuracy: 0.6905 - val_loss: 0.8801 - val_accuracy: 0.5714\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8146 - accuracy: 0.6905 - val_loss: 0.8253 - val_accuracy: 0.5714\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7514 - accuracy: 0.6905 - val_loss: 0.7694 - val_accuracy: 0.5714\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.6905 - val_loss: 0.7097 - val_accuracy: 0.5714\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6303 - accuracy: 0.6905 - val_loss: 0.6586 - val_accuracy: 0.5714\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5812 - accuracy: 0.6905 - val_loss: 0.6140 - val_accuracy: 0.5714\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5427 - accuracy: 0.6905 - val_loss: 0.5707 - val_accuracy: 0.6190\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5104 - accuracy: 0.7024 - val_loss: 0.5441 - val_accuracy: 0.6190\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.7500 - val_loss: 0.5104 - val_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.7619 - val_loss: 0.4970 - val_accuracy: 0.6190\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.7976 - val_loss: 0.4662 - val_accuracy: 0.8095\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8571 - val_loss: 0.4509 - val_accuracy: 0.8095\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8929 - val_loss: 0.4275 - val_accuracy: 0.9048\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3809 - accuracy: 0.9286 - val_loss: 0.4104 - val_accuracy: 0.9048\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.9405 - val_loss: 0.3902 - val_accuracy: 0.9524\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.9643 - val_loss: 0.3752 - val_accuracy: 0.9524\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.9524 - val_loss: 0.3714 - val_accuracy: 0.9048\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.9643 - val_loss: 0.3424 - val_accuracy: 0.9524\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.9524 - val_loss: 0.3427 - val_accuracy: 0.9524\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.9643 - val_loss: 0.3117 - val_accuracy: 0.9524\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.9643 - val_loss: 0.3109 - val_accuracy: 0.9524\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.9643 - val_loss: 0.3022 - val_accuracy: 0.9524\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2715 - accuracy: 0.9643 - val_loss: 0.2770 - val_accuracy: 0.9524\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.9643 - val_loss: 0.2988 - val_accuracy: 0.9524\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.2512 - accuracy: 0.9643 - val_loss: 0.2581 - val_accuracy: 0.9524\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.9643 - val_loss: 0.2533 - val_accuracy: 0.9524\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.9643 - val_loss: 0.2423 - val_accuracy: 0.9524\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9643 - val_loss: 0.2441 - val_accuracy: 0.9524\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9643 - val_loss: 0.2272 - val_accuracy: 0.9524\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2169 - accuracy: 0.9643 - val_loss: 0.2316 - val_accuracy: 0.9524\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9643 - val_loss: 0.2058 - val_accuracy: 0.9524\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.9643 - val_loss: 0.2090 - val_accuracy: 0.9524\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.9643 - val_loss: 0.1968 - val_accuracy: 0.9524\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.9643 - val_loss: 0.2029 - val_accuracy: 0.9524\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9643 - val_loss: 0.1826 - val_accuracy: 0.9524\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9643 - val_loss: 0.1804 - val_accuracy: 0.9524\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1726 - accuracy: 0.9643 - val_loss: 0.1928 - val_accuracy: 0.9524\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9643 - val_loss: 0.1666 - val_accuracy: 0.9524\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9643 - val_loss: 0.1787 - val_accuracy: 0.9524\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9643 - val_loss: 0.1588 - val_accuracy: 0.9524\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9643 - val_loss: 0.1655 - val_accuracy: 0.9524\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1515 - accuracy: 0.9643 - val_loss: 0.1550 - val_accuracy: 0.9524\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9643 - val_loss: 0.1462 - val_accuracy: 0.9524\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9643 - val_loss: 0.1469 - val_accuracy: 0.9524\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9643 - val_loss: 0.1455 - val_accuracy: 0.9524\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9643 - val_loss: 0.1403 - val_accuracy: 0.9524\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9881 - val_loss: 0.1365 - val_accuracy: 0.9524\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9643 - val_loss: 0.1426 - val_accuracy: 0.9524\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9643 - val_loss: 0.1295 - val_accuracy: 0.9524\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9643 - val_loss: 0.1380 - val_accuracy: 0.9524\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9643 - val_loss: 0.1243 - val_accuracy: 0.9524\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9881 - val_loss: 0.1187 - val_accuracy: 0.9524\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9762 - val_loss: 0.1328 - val_accuracy: 0.9524\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9643 - val_loss: 0.1171 - val_accuracy: 0.9524\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9643 - val_loss: 0.1158 - val_accuracy: 0.9524\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9643 - val_loss: 0.1193 - val_accuracy: 0.9524\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1220 - accuracy: 0.9762 - val_loss: 0.1104 - val_accuracy: 0.9524\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9643 - val_loss: 0.1100 - val_accuracy: 0.9524\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.9643 - val_loss: 0.1075 - val_accuracy: 0.9524\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1193 - accuracy: 0.9643 - val_loss: 0.1046 - val_accuracy: 0.9524\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9643 - val_loss: 0.1035 - val_accuracy: 0.9524\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.9643 - val_loss: 0.1079 - val_accuracy: 0.9524\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9881 - val_loss: 0.0953 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9643 - val_loss: 0.1097 - val_accuracy: 0.9524\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.9643 - val_loss: 0.0938 - val_accuracy: 0.9524\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9881 - val_loss: 0.0955 - val_accuracy: 0.9524\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9643 - val_loss: 0.1077 - val_accuracy: 0.9524\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9643 - val_loss: 0.0940 - val_accuracy: 0.9524\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.9643 - val_loss: 0.0897 - val_accuracy: 0.9524\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9881 - val_loss: 0.0875 - val_accuracy: 0.9524\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1011 - accuracy: 0.9762 - val_loss: 0.1038 - val_accuracy: 0.9524\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9762 - val_loss: 0.0860 - val_accuracy: 0.9524\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9762 - val_loss: 0.0888 - val_accuracy: 0.9524\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9643 - val_loss: 0.0880 - val_accuracy: 0.9524\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9762 - val_loss: 0.0925 - val_accuracy: 0.9524\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.0992 - accuracy: 0.9643 - val_loss: 0.0919 - val_accuracy: 0.9524\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9643 - val_loss: 0.0879 - val_accuracy: 0.9524\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0938 - accuracy: 0.9643 - val_loss: 0.0777 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.1082 - accuracy: 0.9762 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9881 - val_loss: 0.1061 - val_accuracy: 0.9524\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9762 - val_loss: 0.0808 - val_accuracy: 0.9524\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.9643 - val_loss: 0.0829 - val_accuracy: 0.9524\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.0971 - accuracy: 0.9643 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9762 - val_loss: 0.0823 - val_accuracy: 0.9524\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 0.0910 - val_accuracy: 0.9524\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9762 - val_loss: 0.0704 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9881 - val_loss: 0.0824 - val_accuracy: 0.9524\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9643 - val_loss: 0.0820 - val_accuracy: 0.9524\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9643 - val_loss: 0.0778 - val_accuracy: 0.9524\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9643 - val_loss: 0.0740 - val_accuracy: 0.9524\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9881 - val_loss: 0.0719 - val_accuracy: 0.9524\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.9643 - val_loss: 0.0719 - val_accuracy: 0.9524\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9762 - val_loss: 0.0717 - val_accuracy: 0.9524\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9643 - val_loss: 0.0789 - val_accuracy: 0.9524\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9762 - val_loss: 0.0656 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Create the MLP\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# declarar forma modelo (Sequencial? Funcional?)\n",
    "model_mlp = Sequential(name='MLP')\n",
    "\n",
    "# adicionar os layers\n",
    "model_mlp.add(Input(X_train.shape[1]))\n",
    "model_mlp.add(Dense(100, activation='relu', kernel_initializer='random_uniform'))\n",
    "model_mlp.add(Dense(3, activation='softmax', kernel_initializer='random_uniform', name='output'))\n",
    "\n",
    "# compilar o modelo: Configuring the learning process\n",
    "model_mlp.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit do modelo\n",
    "history_mlp = model_mlp.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9778\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9905\n",
      "\n",
      "Train results - Loss: 0.08 - Accuracy: 99.0%\n",
      "\n",
      "Test results - Loss: 0.09 - Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training using evaluate\n",
    "test_results = model_mlp.evaluate(X_test, y_test)\n",
    "train_results = model_mlp.evaluate(X_train, y_train)\n",
    "\n",
    "print(f'\\nTrain results - Loss: {train_results[0]:.2f} - Accuracy: {100*train_results[1]:.1f}%')\n",
    "print(f'\\nTest results - Loss: {test_results[0]:.2f} - Accuracy: {100*test_results[1]:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MLP\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               500       \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 803\n",
      "Trainable params: 803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "# Primeira camada\n",
    "# 4 neurônios de input * 100 neurônios ocultos + 100 bias\n",
    "print(4*100 + 100)\n",
    "\n",
    "# Camada output\n",
    "# 100 neurônios ocultos * 3 neurônios de output + 3 bias\n",
    "print(100*3 + 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saber quantidade de parâmetros: \n",
    "model_mlp.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MLP',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 4),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_6'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 100,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'RandomUniform',\n",
       "     'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'output',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 3,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'RandomUniform',\n",
       "     'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pegar estrutura da rede neural\n",
    "model_mlp.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saber pesos\n",
    "# model_mlp.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferência: Usando um modelo após o treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.76643482e-07, 4.42222171e-02, 9.55777645e-01],\n",
       "       [4.31829481e-04, 8.95249128e-01, 1.04319014e-01],\n",
       "       [7.02467732e-05, 5.82698464e-01, 4.17231351e-01],\n",
       "       [4.43396566e-04, 9.36080396e-01, 6.34762421e-02],\n",
       "       [3.44254440e-05, 3.97100359e-01, 6.02865100e-01],\n",
       "       [3.00034735e-08, 1.00918775e-02, 9.89908099e-01],\n",
       "       [1.08703435e-03, 8.79743755e-01, 1.19169310e-01],\n",
       "       [4.29078564e-03, 9.81856883e-01, 1.38523243e-02],\n",
       "       [9.96851027e-01, 3.14898649e-03, 3.71234467e-11],\n",
       "       [1.94394875e-06, 1.23957083e-01, 8.76040995e-01],\n",
       "       [9.99196827e-01, 8.03147093e-04, 3.28299914e-13],\n",
       "       [9.94519889e-01, 5.47999656e-03, 4.96576946e-10],\n",
       "       [1.26623362e-07, 2.13682670e-02, 9.78631556e-01],\n",
       "       [3.78839422e-06, 1.61423653e-01, 8.38572562e-01],\n",
       "       [9.97484803e-01, 2.51513440e-03, 2.24898624e-11],\n",
       "       [2.71652453e-07, 3.19252722e-02, 9.68074441e-01],\n",
       "       [1.85947772e-02, 9.75445271e-01, 5.95997693e-03],\n",
       "       [9.98194516e-01, 1.80548697e-03, 6.36270160e-12],\n",
       "       [9.86931086e-01, 1.30687905e-02, 1.62292046e-09],\n",
       "       [9.98069406e-01, 1.93055114e-03, 6.75921688e-12],\n",
       "       [1.87280471e-03, 9.84030485e-01, 1.40967080e-02],\n",
       "       [9.96906519e-01, 3.09355743e-03, 4.98076996e-11],\n",
       "       [1.43438205e-03, 9.87153053e-01, 1.14125647e-02],\n",
       "       [2.18483240e-08, 7.66141061e-03, 9.92338538e-01],\n",
       "       [4.57693714e-05, 4.30841476e-01, 5.69112837e-01],\n",
       "       [9.64723644e-04, 9.77203012e-01, 2.18322389e-02],\n",
       "       [3.67545203e-04, 7.68398106e-01, 2.31234446e-01],\n",
       "       [1.64061300e-02, 9.76967335e-01, 6.62651379e-03],\n",
       "       [4.33709123e-04, 8.31103802e-01, 1.68462455e-01],\n",
       "       [9.97112930e-01, 2.88702943e-03, 4.10571022e-11],\n",
       "       [4.73296586e-06, 1.39584750e-01, 8.60410511e-01],\n",
       "       [2.21398864e-08, 6.60992088e-03, 9.93390083e-01],\n",
       "       [7.51903804e-04, 9.55512464e-01, 4.37356494e-02],\n",
       "       [9.96872067e-01, 3.12802778e-03, 5.26165361e-11],\n",
       "       [1.10779740e-06, 6.53667599e-02, 9.34632063e-01],\n",
       "       [9.97318983e-01, 2.68112239e-03, 3.22460912e-11],\n",
       "       [9.95532691e-01, 4.46734065e-03, 2.42871750e-10],\n",
       "       [9.96242166e-01, 3.75790359e-03, 1.16155995e-10],\n",
       "       [9.89334047e-01, 1.06658405e-02, 1.09494169e-09],\n",
       "       [9.00708401e-05, 7.27584600e-01, 2.72325367e-01],\n",
       "       [7.32529501e-04, 9.05867577e-01, 9.33999866e-02],\n",
       "       [9.97982860e-01, 2.01708172e-03, 1.43558863e-11],\n",
       "       [7.47647528e-06, 7.93272927e-02, 9.20665205e-01],\n",
       "       [3.70762996e-06, 1.49262190e-01, 8.50734055e-01],\n",
       "       [1.06026256e-03, 9.81422305e-01, 1.75175220e-02]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 2, 2, 1, 1, 0, 2, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 1, 0,\n",
       "       1, 2, 2, 1, 1, 1, 1, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 2, 2,\n",
       "       1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_pred_mod = np.argmax(y_pred, axis=1)\n",
    "y_pred_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 0, 15,  0],\n",
       "       [ 0,  1, 14]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot da função de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='val')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o modelo converge e tanto a performance de treino e de teste ficam equivalentes. Essa performance e convergência sugerem que escolhemos uma boa função de custo para nosso modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo:\n",
    "\n",
    "Neste exemplo, usaremos o **MNIST**, o famoso dataset de dígitos (números de 0 a 9) escritos à mão. O objetivo do nosso modelo será o de classificar digítos, com base em imagens. Assim sendo, temos um **problema de classificação multiclasse** (pois os dados serão classificados em uma dentre 10 classes possíveis, de 0 a 9).\n",
    "\n",
    "Primeiramente, lemos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa libs\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa mnist\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dos dados de treino e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 6, 9, 7], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limita tamanho dos dados\n",
    "X_train[:10000]\n",
    "y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que acima já temos separados os dados de treino e de teste!\n",
    "\n",
    "Pra garantir maior rapidez do algoritmo, **reescalamos** as **features** para que tenham valores entre 0 e 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para reescalar as features, as dividimos pelo valor máximo das features de treino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos converter nossos targets em matrizes com a mesma quantidade de linhas que anteriormente, mas agora com uma coluna para cada classe utilizando o to_categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos separar nossos dados de treino em um conjunto para treino e outro para validação. No Keras podemos tanto selecionar uma porcentagem dos dados de treino para validação, quanto passar um conjunto de validação pré-determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, definimos a arquitetura de nossa rede neural, usando o Keras!\n",
    "\n",
    "Vamos construir uma rede neural simples, com 3 camadas ocultas densas e 25 neurônios em cada camada, com a função de ativação **ReLu**. Para a camada de output, usamos a **Softmax**, dado que temos um problema de classificação multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de funções úteis do Keras\n",
    "\n",
    "\n",
    "# definição da arquitetura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, compilamos a rede neural, explicitando o otimizador e a função de perda desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o otimizador RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dois parâmetros na função acima são importantes:\n",
    "- **\"loss\"**, que é a função de perda a ser minimizada. Esta função depende do tipo de problema que queremos resolver. Como temos um problema de classificação multiclasse, a função de perda adequada é ou a \"categorical_crossentropy\" ou a \"sparse_categorical_crossentropy\".\n",
    "- **\"optimizer\"**, que é o otimizador que utilizaremos para minimizar a função de perda. A escolha do otimizador é bem mais livre: é uma boa ideia testar diferentes otimizadores! Os mais utilizados são: **Adam**, **SGD** e **RMSprop**.\n",
    "\n",
    "Por fim, treinamos a rede neural! Basta usar o método \".fit()\", determinando o número de epochs bem como os dados de treino e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, pronto! A rede neural está treinada!\n",
    "\n",
    "Como em todo problema de classificação, podemos utilizar o classification report para avaliar sua performance nos dados de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo as predições\n",
    "\n",
    "\n",
    "# Extrai classe com maior probabilidade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe o classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso a performance não seja suficiente, entram os próximos passos, muito comuns em todo projeto de Deep Learning:\n",
    "\n",
    "- Tentar arquiteturas diferentes de Redes Neurais;\n",
    "- Tunar os hiperparâmetros;\n",
    "- Mudar o otimizador;\n",
    "- Investigar a ocorrência de overfitting, e usar técnicas de regularização;\n",
    "\n",
    "...entre outras!\n",
    "\n",
    "Lembre-se: a construção de um modelo de Deep Learning é um processo altamente **iterativo**, de tentativa e erro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que a performance de treino não mudou, mas pioramos nossa generalização no teste\n",
    "_____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Referências**\n",
    "\n",
    "https://playground.tensorflow.org/\n",
    "\n",
    "https://www.deeplearningbook.com.br\n",
    "\n",
    "https://keras.io\n",
    "\n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "https://www.louisbouchard.ai/densenet-explained/\n",
    "\n",
    "https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html\n",
    "\n",
    "https://valueml.com/multi-layer-perceptron-by-keras-with-example/\n",
    "\n",
    "https://ml4a.github.io/ml4a/how_neural_networks_are_trained/\n",
    "\n",
    "[Três formas de montar redes neurais com o Keras](https://developpaper.com/three-methods-of-constructing-neural-network-with-keras/)\n",
    "\n",
    "[Álgebra Linear](https://mlfromscratch.com/tag/linear-algebra/)\n",
    "\n",
    "[Comparação entre otimizadores](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
    "\n",
    "[Binary Cross Entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "\n",
    "[Derivada no backpropagation: exemplo demostrando Regra da Cadeia](https://www.jeremyjordan.me/neural-networks-training/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercício**\n",
    "\n",
    "Você trabalhará com o conjunto de dados da Reuters, um conjunto de notícias curtas e seus tópicos, publicados pela Reuters em 1986. É um conjunto de dados de teste simples e amplamente utilizado para classificação de texto. Nele encontramos 46 temas diferentes com alguns tópicos aparecendo mais do que outros, mas cada tópico tem pelo menos 10 exemplos no conjunto de treinamento.\n",
    "\n",
    "Esse dataset foi originalmente gerado pela análise e pré-processamento do conjunto de dados clássico Reuters-21578. Cada noticia é codificada como uma lista de índices de palavras (inteiros). Por conveniência, as palavras são indexadas pela frequência geral no conjunto de dados, de modo que, por exemplo, o inteiro \"3\" codifica a 3ª palavra mais frequente nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe os dados do `tensorflow.keras.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) # num_words=10000 restringe os dados as 10.000 palavras mais frequentes encontradas nos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual o shape dos datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecione o primeiro elemento de train_data e retorne seu tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecione o terceiro elemento de train_data e retorne seu tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que cada linha do dataset de treino tem um tamanho diferente, que pré-processamento podemos fazer para deixar todas com o mesmo tamanho?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta: adicionar zeros até chegar em um valor de coluna limite, limitar a quantidade para a menor, ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sugestão de pré-processamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    \"\"\"Multi-hot encode your lists to turn them into vectors of 0s and 1s. This would\n",
    "    mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vec-\n",
    "    tor that would be all 0s except for indices 8 and 5, which would be 1s. Then you\n",
    "    could use a Dense layer, capable of handling floating-point vector data, as the\n",
    "    first layer in your model\"\"\"\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize o método `to_categorical` do `tensorflow.keras.utils` ou o `LabelBinarizer` do `sklearn.preprocessing` para converter os labels de uma coluna para n colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar a montar nossa rede neural!\n",
    "\n",
    "- Quantas camadas ocultas colocar?\n",
    "- Quantos neurônios em cada camada oculta?\n",
    "- Qual será a função de cada camada oculta?\n",
    "- Quantos neurônios na camada de saída?\n",
    "- Qual a função na cada camada de saída?\n",
    "- Meu problema é de classificação ou regressão? Se for de classificação, é binário ou multi-classe?\n",
    "- Qual função custo utilizar?\n",
    "- Qual métrica para avaliar meu modelo?\n",
    "- Qual otimizador usar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plote as curvas da função custo e sua métrica por epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu modelo está overfitando, underfitando ou está bom? Porquê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seu modelo não ficou bom, o que você pode fazer para melhorá-lo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare a acurácia entre as seguintes redes neurais:\n",
    "\n",
    "1. Uma camada densa com 64 neurônios > uma camada densa com 64 neurônios > uma camada densa com 46 neurônios\n",
    "2. Uma camada densa com 64 neurônios > uma camada densa com 4 neurônios > uma camada densa com 46 neurônios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta: The model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop is\n",
    "mostly due to the fact that we’re trying to compress a lot of information (enough\n",
    "information to recover the separation hyperplanes of 46 classes) into an intermediate\n",
    "space that is too low-dimensional. The model is able to cram most of the necessary\n",
    "information into these four-dimensional representations, but not all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça outros testes diminuindo e aumentando tanto a quantidade de neurônios quanto a de camadas ocultas.\n",
    "\n",
    "### Fim!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "_____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprofundamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: Binary Cross-Entropy Loss / Log Loss\n",
    "\n",
    "This is the most common loss function used in classification problems. The cross-entropy loss decreases as the predicted probability converges to the actual label. It measures the performance of a classification model whose predicted output is a probability value between 0 and 1.\n",
    "\n",
    "<img src=https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_common-loss-functions_1.png width=400 text=\"https://builtin.com/machine-learning/common-loss-functions\">\n",
    "\n",
    "When the number of classes is 2, it’s binary classification.\n",
    "common-loss-functions\n",
    "\n",
    "<img src=https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/2_common-loss-functions.png width=200 text=\"https://builtin.com/machine-learning/common-loss-functions\" >\n",
    "\n",
    "When the number of classes is more than 2, it’s multi-class classification.\n",
    "common-loss-functions\n",
    "\n",
    "<img src=https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_common-loss-functions.png width=500 text=\"https://builtin.com/machine-learning/common-loss-functions\">\n",
    "\n",
    "We derive the cross-entropy loss formula from the regular likelihood function, but with logarithms added in.\n",
    "\n",
    "Extraído de https://builtin.com/machine-learning/common-loss-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notação matricial das equações\n",
    "\n",
    "Denotamos cada ativação por $a_{neuron}^{(layer)}$, por exemplo $a^{(1)}_2$ corresponderia ao neurônio número três na segunda camada (contamos a partir de 0). Assim, o número abaixo (subscrito) corresponde a qual neurônio estamos falando, e o número acima (sobrescrito) corresponde a qual camada estamos olhando, ambos contando a partir de zero.\n",
    "\n",
    "Denotamos cada peso por $w_{to, from}^{(layer)}$, onde $to$ é denotado como $j$ e $from$ denotado como $k$, assim,  $w^2_{2,3}$ significa indo para o terceiro neurônio na terceira camada, a partir do neurônio quatro na camada anterior (segunda camada), pois contamos a partir do zero.\n",
    "\n",
    "Exemplo de vetor de ativação para a camada zero:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    a_0^{0}\\\\\n",
    "    a_1^{0}\\\\\n",
    "    \\vdots \\\\\n",
    "    a_n^{0}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "Exemplo de matriz de pesos que conectam cada neuron a próxima camada:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    w_{0,0} & w_{0,1} & \\cdots & w_{0,k}\\\\\n",
    "    w_{1,0} & w_{1,1} & \\cdots & w_{1,k}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{j,0} & w_{j,1} & \\cdots & w_{j,k}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Combinando esses dois com a matriz de bias e encapsulando na função sigmóide:\n",
    "\n",
    "$$\n",
    "\\sigma \\left(\n",
    "    \\begin{bmatrix}\n",
    "    w_{0,0} & w_{0,1} & \\cdots & w_{0,k}\\\\\n",
    "    w_{1,0} & w_{1,1} & \\cdots & w_{1,k}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{j,0} & w_{j,1} & \\cdots & w_{j,k}\\\\\n",
    "    \\end{bmatrix}\n",
    "    \\, \n",
    "    \\begin{bmatrix}\n",
    "    a_0^{0}\\\\\n",
    "    a_1^{0}\\\\\n",
    "    \\vdots \\\\\n",
    "    a_n^{0}\\\\\n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "    b_0\\\\\n",
    "    b_1\\\\\n",
    "    \\vdots \\\\\n",
    "    b_n\\\\\n",
    "    \\end{bmatrix}\n",
    "    \\right)\n",
    "    $$\n",
    "\n",
    "Podemos escrever de outra forma:\n",
    "\n",
    "$$ a^{(1)}=\n",
    "\\sigma\\left(\n",
    "\\boldsymbol{W}\\boldsymbol{a}^{0}+\\boldsymbol{b}\n",
    "\\right) $$\n",
    "\n",
    "Ou reduzir a notação ainda mais e escrever:\n",
    "\n",
    "$$ a^{(1)}=\n",
    "\\sigma\\left(\n",
    "\\boldsymbol{z}\n",
    "\\right)$$\n",
    "\n",
    "Com isso conseguimos a matriz de todas as ativações da segunda camada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb5f626699f206ef97176a4f092b8d9f6e52ae1f84b4bb3163daf9eb25ca3519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
