{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOFXV4l55G3HiwE8fYgBrjN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Aula 3 - Processamento de Linguagem Natural - Spacy e Word2Vec\n","\n","Na aula de hoje iremos explorar os seguintes tópicos:\n","- Spacy\n","- Word2Vec\n"],"metadata":{"id":"UfqF9ea7oziQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcXRnIdDowdh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661818319593,"user_tz":180,"elapsed":96228,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"c2c524b5-f61f-42e8-ec97-92c079ff1447"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","2022-08-30 00:10:36.083632: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pt-core-news-lg==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.4.0/pt_core_news_lg-3.4.0-py3-none-any.whl (568.2 MB)\n","\u001b[K     |████████████████████████████████| 568.2 MB 18 kB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-lg==3.4.0) (3.4.1)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (4.1.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (57.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (21.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.10)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.7)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (8.1.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.4.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.11.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.0.3)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.4.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.21.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.9.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (4.64.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-lg==3.4.0) (2.0.1)\n","Installing collected packages: pt-core-news-lg\n","Successfully installed pt-core-news-lg-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_lg')\n","2022-08-30 00:11:25.879872: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pt-core-news-md==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.4.0/pt_core_news_md-3.4.0-py3-none-any.whl (42.4 MB)\n","\u001b[K     |████████████████████████████████| 42.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-md==3.4.0) (3.4.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.8)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.9.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (57.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (4.64.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.4.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.23.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.0.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.10.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.6)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (8.1.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.10)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.6.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (4.1.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.11.3)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.4.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-md==3.4.0) (2.0.1)\n","Installing collected packages: pt-core-news-md\n","Successfully installed pt-core-news-md-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_md')\n","2022-08-30 00:11:36.846400: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pt-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n","\u001b[K     |████████████████████████████████| 13.0 MB 30.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: pt-core-news-sm\n","Successfully installed pt-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_sm')\n","2022-08-30 00:11:44.784552: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 28.7 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Collecting gensim\n","  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[K     |████████████████████████████████| 24.1 MB 87.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 31.0 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.4\n"]}],"source":["!pip install spacy -U\n","!python -m spacy download pt_core_news_lg\n","!python -m spacy download pt_core_news_md\n","!python -m spacy download pt_core_news_sm\n","!python -m spacy download en_core_web_sm\n","# Instalando a biblioteca gensim para trabalhar com word2vec e doc2vec\n","!pip install -U gensim\n","!pip install unidecode"]},{"cell_type":"code","source":["# Importando o numpy para manipulação de vetores\n","import numpy as np\n","# Manipulação de tabelas\n","import pandas as pd\n","# Expressões regulares\n","import re\n","# Importando o unidecode\n","from unidecode import unidecode\n","# Biblioteca de processamento de linguagem natural\n","import nltk\n","from nltk.corpus import stopwords \n","from nltk.stem.porter import *\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer \n","# Importanto o extrator de features de texto CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Para visualizar or dados\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# Tipagem\n","from typing import List\n","# Importanto o gensim.downloader para baixar os dados da wiki\n","import gensim.downloader as api\n","# Baixando as stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')  # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n","nltk.download('rslp')  # Stemmer em português\n","\n","# https://www.nltk.org/howto/wordnet.html\n","nltk.download('wordnet')\n","\n","# NLTK 3.6.6 release: December 2021:\n","# support OMW 1.4, use Multilingual Wordnet Data from OMW with newer Wordnet versions\n","nltk.download('omw-1.4')\n","\n","wv_wiki = api.load('glove-wiki-gigaword-300')"],"metadata":{"id":"t1EOEzE_pBQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661818510132,"user_tz":180,"elapsed":190542,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"48834e5b-4d77-4de9-bdd7-db3fb4848c01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Unzipping stemmers/rslp.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"]}]},{"cell_type":"code","source":["# Carregando os dados de tweets sobre desastres naturais\n","tweets = pd.read_csv('./9.3.tweets.csv', index_col=0)"],"metadata":{"id":"cTykjcJkq1Lc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explorando as colunas e dados de tweets\n","tweets.head()"],"metadata":{"id":"ML4KNjpmq1It","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1661819078255,"user_tz":180,"elapsed":524,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"8f11f500-c4ed-480b-bc61-b8146e32619f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   keyword location                                               text  target\n","id                                                                            \n","1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n","4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n","5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n","6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n","7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"],"text/html":["\n","  <div id=\"df-f6a1e6cb-acea-4738-915f-a54e0576d3a2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6a1e6cb-acea-4738-915f-a54e0576d3a2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f6a1e6cb-acea-4738-915f-a54e0576d3a2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f6a1e6cb-acea-4738-915f-a54e0576d3a2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Verficando o tipo de cada coluna\n","# e a presença de elementos nulos no conjunto de dados\n","tweets.info()\n","# A coluna text refere-se ao texto do tweet\n","# A coluna target se o tweet é referente a um desastre (1) ou não (0)"],"metadata":{"id":"n_eK4Qz1q1F7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661819109708,"user_tz":180,"elapsed":2,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"0ae05360-a53b-4df8-9131-8ff0b124b18d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 7613 entries, 1 to 10873\n","Data columns (total 4 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   keyword   7552 non-null   object\n"," 1   location  5080 non-null   object\n"," 2   text      7613 non-null   object\n"," 3   target    7613 non-null   int64 \n","dtypes: int64(1), object(3)\n","memory usage: 297.4+ KB\n"]}]},{"cell_type":"code","source":["# Temos mais dados sobre não desastres do que de desastres\n","tweets['target'].value_counts()"],"metadata":{"id":"J4WWtttiq1DM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661819240926,"user_tz":180,"elapsed":2,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"b52dab6a-628c-4632-8726-4959e7a258f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    4342\n","1    3271\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Classe para o pre-processamento de dados\n","class PreProcessPhrase:\n","\n","  def remove_acentuacao(self, phrase: str, debug: bool = False) -> str:\n","    # Utilizando a biblioteca unidecode para remover acentuação de texto\n","    phrase_fmt = unidecode(phrase)\n","    if debug:\n","      print('`remove_acentuacao`: Frase original', phrase)\n","      print('`remove_acentuacao`: Frase formatada', phrase_fmt)\n","\n","    # Retornando a frase formatada\n","    return phrase_fmt\n","\n","\n","  def remove_digits(self, phrase: str, debug: bool = False) -> str:\n","    # utilizando expressões regulares para remoção de digitos\n","    phrase_no_digits = re.sub(r'\\d', '', phrase)\n","\n","    # Se quisermos ligar o debug, mostre a frase original e transformada\n","    if debug:\n","      print('`remove_digits`: Texto original:', phrase)\n","      print('`remove_digits`: Texto sem digitos:', phrase_no_digits)\n","\n","    # Retornando a frase sem digitos\n","    return phrase_no_digits\n","\n","  def remove_special_char(self, phrase: str, debug: bool = False) -> str:\n","    # utilizando expressões regulares para remoção de caracteres especiais\n","    phrase_no_special_char = re.sub(r'[^a-zA-Z0-9]+', ' ', phrase)\n","\n","    # Se quisermos ligar o debug, mostre a frase original e transformada\n","    if debug:\n","      print('`remove_special_char`: Texto original:', phrase)\n","      print('`remove_special_char`: Texto sem caracteres especiais:', phrase_no_special_char)\n","\n","    # Retornando a frase sem digitos\n","    return phrase_no_special_char\n","\n","  def word_lower(self, word: str, debug: bool = False) -> str:\n","    try:\n","      # Formatando a palavra em caixa baixa\n","      word_fmt = word.lower()\n","\n","      # Se o debug for True, iremos imprimir a palavra original e transformada\n","      if debug:\n","        print('`word_lower`: Palavra Original:', word)\n","        print('`word_lower`: Palavra transformada:', word_fmt)\n","\n","    except:\n","      # Caso a palavra não seja uma string levante um erro (TypeError) informando qual o tipo da palavra passada\n","      raise TypeError(f'Esperava uma `word` no tipo str, foi passado uma {type(word)}')\n","\n","    # Retornando a palavra formatada\n","    return word_fmt\n","\n","  def remove_stopwords(self, words: List[str], debug=False) -> List[str]:\n","    # Carregando as stopwords (inglês)\n","    stopwords_en = stopwords.words('english')\n","    # Criando uma váriavel que irá armazenar elementos limpos, que não estejam dentro das stopwords\n","    clean_words = []\n","\n","    # Percorrendo cada palavra da nossa lista de palavras\n","    for word in words:\n","      # Verificando se a palavra não está presente das stopwords\n","      if word not in stopwords_en:\n","        # Se a palavra não é uma stopword adicionamos elas a váriavel clean_words\n","        clean_words.append(word)\n","      else:\n","        # Caso a palavra seja uma stopword e estamos no modo de debug (debug=True)\n","        if debug:\n","          # Imprimimos qual a palavra da lista words é uma stopword\n","          print(f'`remove_stopwords`: A palavra {word} está presente nas stopwords')\n","    return clean_words\n","\n","  def tokenizer(self, phrase: str, debug: bool) -> List[str]:\n","    words = word_tokenize(phrase)\n","    if debug:\n","        print('`tokenizer`: Frase original:', phrase)\n","        print('`tokenizer`: tokens:', words)\n","    return words\n","\n","  def stemmer(self, words: List[str], debug: bool = False) -> List[str]:\n","    # Inicializando o Porter Stemmer (inglês)\n","    stemmer = PorterStemmer()\n","    # Criando uma lista vazia para armazenar as palavras stem\n","    stem_words = []\n","    for word in words:\n","      # Pegando o stem de cada palavra\n","      s_word = stemmer.stem(word)\n","      # Adicionando essa palavra modificada a lista stem_words\n","      stem_words.append(s_word)\n","\n","    if debug:\n","        print('`stemmer`: Tokens originais:', words)\n","        print('`stemmer`: Tokens transformadods:', stem_words)\n","    return stem_words\n","\n","  def lemmatizer(self, words: List[str], debug: bool = False) -> List[str]:\n","    # Inicializando o WordNetLemmatizer (inglês)\n","    lemmatizer = WordNetLemmatizer()\n","    # Criando uma lista vazia para armazenar as palavras lemma\n","    lemm_words = []\n","    for word in words:\n","      # Pegando o lemma de cada palavra\n","      l_words = lemmatizer.lemmatize(word, pos='v')\n","      # Adicionando essa palavra modificada (lemma) a lista lemm_words\n","      lemm_words.append(l_words)\n","\n","    if debug:\n","        print('`lemmatizer`: Tokens originais:', words)\n","        print('`lemmatizer`: Tokens transformadods:', lemm_words)\n","    return lemm_words\n","\n","\n","  def pipeline(self, phrase: str, methods: List[str], debug: bool = False):\n","    switcher = {\n","        'remove_acentuacao': self.remove_acentuacao,\n","        'remove_digits': self.remove_digits,\n","        'remove_special_char': self.remove_special_char,\n","        'word_lower': self.word_lower,\n","        'remove_stopwords': self.remove_stopwords,\n","        'tokenizer': self.tokenizer,\n","        'stemmer': self.stemmer,\n","        'lemmatizer': self.lemmatizer\n","    }\n","    for method in methods:\n","      # remove_stopwords\n","      if method == 'remove_stopwords':\n","        phrase = switcher[method](phrase, debug=debug)\n","        \n","      else:\n","        phrase = switcher[method](phrase, debug=debug)\n","\n","    return phrase\n","\n"],"metadata":{"id":"27F2deV3sftN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicando o preprocessamento de dados\n","# Instânciando o preprocessador de frases\n","preprocess = PreProcessPhrase()\n","# Definindo os passos da nossa pipeline\n","pipeline = [\n","    'remove_digits',\n","    'remove_special_char',\n","    'word_lower',\n","    'tokenizer',\n","    'remove_stopwords',\n","    'stemmer'\n","]\n","# Aplicando a pipeline de preprocessamento para cada documento (linha)\n","tweets[\"filtered_words\"] = tweets['text'].apply(preprocess.pipeline, methods=pipeline)\n","\n","# Normalmente depois do processamento juntamos as palavras novamente em uma só string\n","tweets['join_words'] = tweets['filtered_words'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"NBIQ4osQsjoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definindo as features e targets\n","X = tweets['join_words']\n","y = tweets['target']"],"metadata":{"id":"DGlVIqqisrz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importando o train_test_split para separar os dados de treino e teste\n","from sklearn.model_selection import train_test_split\n","\n","# Separando os dados de treino e teste\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                     y, \n","                                                     test_size = 0.3, \n","                                                     random_state = 42)"],"metadata":{"id":"DD6oEwyyuBoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"omB2MiNHuEp5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### N-gramas"],"metadata":{"id":"xolYbHJHtAnh"}},{"cell_type":"code","source":["frase = \"13.000 pessoas receberam ordens de evacuação por #incêndios na California\""],"metadata":{"id":"DKiRM4V-sjmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicando o preprocessamento de dados\n","# Instânciando o preprocessador de frases\n","preprocess = PreProcessPhrase()\n","pipeline = [\n","    'word_lower',\n","    'tokenizer',\n","    'remove_stopwords',\n","    'stemmer'\n","]\n","# Aplicando o preprocessamento na frase `frase`\n","tokens = preprocess.pipeline(frase, methods=pipeline)\n"],"metadata":{"id":"g3WQI4tgsjje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imprimindo os unigramas\n","tokens"],"metadata":{"id":"yIPoyZ9Nsjg9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661819452581,"user_tz":180,"elapsed":2,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"60f42852-426e-4bd3-db5d-f9aeea02111a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['13.000',\n"," 'pessoa',\n"," 'receberam',\n"," 'orden',\n"," 'de',\n"," 'evacuação',\n"," 'por',\n"," '#',\n"," 'incêndio',\n"," 'na',\n"," 'california']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Imprimindo os bigramas\n","print(list(nltk.bigrams(tokens)))"],"metadata":{"id":"ixwwM3hXsjeQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661819495783,"user_tz":180,"elapsed":1,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"04ee7d83-9f5a-4570-f969-82a8f140b8bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('13.000', 'pessoa'), ('pessoa', 'receberam'), ('receberam', 'orden'), ('orden', 'de'), ('de', 'evacuação'), ('evacuação', 'por'), ('por', '#'), ('#', 'incêndio'), ('incêndio', 'na'), ('na', 'california')]\n"]}]},{"cell_type":"code","source":["# Imprimindo os trigramas\n","print(list(nltk.trigrams(tokens)))"],"metadata":{"id":"uiRH8AEgsjbo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661819585300,"user_tz":180,"elapsed":258,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"d982bdd1-4e4a-483f-ecf3-1e43d2c046ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('13.000', 'pessoa', 'receberam'), ('pessoa', 'receberam', 'orden'), ('receberam', 'orden', 'de'), ('orden', 'de', 'evacuação'), ('de', 'evacuação', 'por'), ('evacuação', 'por', '#'), ('por', '#', 'incêndio'), ('#', 'incêndio', 'na'), ('incêndio', 'na', 'california')]\n"]}]},{"cell_type":"markdown","source":["**Drops**\n","\n","Utilize o CountVectorizer e verifique como ficam as features quando utilizamos unigramas e bigramas no dataset de tweets"],"metadata":{"id":"T5mxwkN7tmXY"}},{"cell_type":"code","source":["# Importanto o extrator de features de texto CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","# Cria apenas unigramas\n","cv_unigrama = CountVectorizer(\n","    # N-gram range (min e max), indica se queremos contar n-gramas\n","    # (1,1) significa apenas unigramas\n","    # (1,2) siginifica unigramas e bigramas\n","\n","    ngram_range=(1,1), # PREENCHA AQUI\n","\n","    # Se esse parâmetro for True, o resultado é binário (0, 1)\n","    # Se falso o retorno é a contagem da palavra na frase, sua ocorrência por frase\n","    binary=False\n",")"],"metadata":{"id":"Bsq-9ldosjY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplique o fit_transform com o cv_unigramas nos dados de treino\n","X_train_cv_unigrama = cv_unigrama.fit_transform(X_train).todense() # PREENCHA AQUI"],"metadata":{"id":"qBRa57E7sjWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verifique o tamanho dos dados (X_train_cv_unigrama), tanto o número de linhas como de features\n","print(X_train_cv_unigrama.shape)"],"metadata":{"id":"s6xz35AmuOIT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661820453249,"user_tz":180,"elapsed":259,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"69c9d599-8233-4dc3-e686-32340e95cd8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(5329, 13803)\n"]}]},{"cell_type":"code","source":["# Imprima as dez primeiras palavras do vocabulário\n","print(cv_unigrama.get_feature_names_out()[:10]) # PREENCHA AQUI\n","# Imprima o tamanho do vocabulário\n","print(cv_unigrama.get_feature_names_out().shape) # PREENCHA AQUI"],"metadata":{"id":"lM5-1OESuQCS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661820521217,"user_tz":180,"elapsed":254,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"1eca092f-c99f-4c48-b4de-f08c18518853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['aa' 'aaaa' 'aaaaaaallll' 'aaarrrgghhh' 'aac' 'aadzvsr' 'aal' 'aamir'\n"," 'aan' 'aannnnd']\n","(13803,)\n"]}]},{"cell_type":"code","source":["# Cria unigramas e bigramas\n","cv_bigrama = CountVectorizer(\n","    # N-gram range (min e max), indica se queremos contar n-gramas\n","    # (1,1) significa apenas unigramas\n","    # (1,2) siginifica unigramas e bigramas\n","\n","    ngram_range=(1, 2), # PREENCHA AQUI\n","\n","    # Se esse parâmetro for True, o resultado é binário (0, 1)\n","    # Se falso o retorno é a contagem da palavra na frase, sua ocorrência por frase\n","    binary=False\n",")"],"metadata":{"id":"NOtceOsHuTnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Aplique o fit_transform com o cv_bigrama nos dados de treino\n","X_train_cv_bigrama = cv_bigrama.fit_transform(X_train) # PREENCHA AQUI"],"metadata":{"id":"l0O9ZDzuuaSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verifique o tamanho dos dados (X_train_cv_bigrama), tanto o número de linhas como de features\n","print(X_train_cv_bigrama.shape)"],"metadata":{"id":"pZB6Jno2uh58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661820648388,"user_tz":180,"elapsed":2,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"ecabea31-41e0-4156-bc85-f9a4c9647450"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(5329, 52162)\n"]}]},{"cell_type":"code","source":["# Imprima as dez primeiras palavras do vocabulário\n","print(cv_bigrama.get_feature_names_out()[:10]) # PREENCHA AQUI\n","# Imprima o tamanho do vocabulário\n","print(cv_bigrama.get_feature_names_out().shape) # PREENCHA AQUI"],"metadata":{"id":"Foccmqksujxe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661820703009,"user_tz":180,"elapsed":445,"user":{"displayName":"Gilberto Kaihami","userId":"07651446941847976228"}},"outputId":"3edc1d78-f8d7-45d0-b9a5-6c4367d222f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['aa' 'aa ayyo' 'aa batteri' 'aa mgm' 'aa near' 'aaaa' 'aaaa ok'\n"," 'aaaaaaallll' 'aaaaaaallll even' 'aaarrrgghhh']\n","(52162,)\n"]}]},{"cell_type":"markdown","source":["**Houve um grande aumento da dimensionalidade utilizando uni e bigramas**\n","\n","Portanto o modelo pode sofrer da maldição da dimensionalidade, para evitar isso, iremos utilizar o word2vec e doc2vec!"],"metadata":{"id":"MY8Am_-guvbz"}},{"cell_type":"markdown","source":["---\n","Sobre lematização na Língua Portuguesa:\n","\n","Iremos utilizar a biblioteca Spacy"],"metadata":{"id":"8SZZhYk_vA6G"}},{"cell_type":"code","source":[],"metadata":{"id":"Wam9cKbmumcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3yDIs2vfvKqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DFuJndQuvQK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wn9dd0PqvR0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aSub23hJveQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X2ERcdlvvkCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-Xdt6KGsvlRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4BaVt-T9vyBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MvR3iyETymdA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YsvKAaenv92P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f1NVSEpcwJ4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8DgJDVT1yISJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7lW_t3WHyH-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Drops**  \n","Além da palavra em si, pelo Spacy utilizar POO, há diversos outros atributos interessantes.  \n","O primeiro é o POS, Part of speech, o segundo é o `dep_` que representa a dependência sintática.\n","\n","Para saber mais acesse: https://spacy.io/usage/linguistic-features\n","\n","Crie dois laços:\n","\n","O primeiro que imprima a partir de cada `Token` a sua string (`text`) e a sua POS (`pos_`).\n","\n","O seguindo laço, a partir de cada `Token` a sua string (`text`) e a sua POS (`pos_`), e a sua dep (`dep_`).\n"],"metadata":{"id":"uaWngyxIwk3I"}},{"cell_type":"code","source":["# Imprima o texto e o POS (`pos_`)\n","for word in doc:\n","  print(word...., word....)  # PREENCHA AQUI"],"metadata":{"id":"Q0kSuUdmwY2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imprima o texto, o POS (`pos_`), e a dependência sintática (`dep_`)\n","for word in doc:\n","  print(word...., word...._, word...)  # PREENCHA AQUI"],"metadata":{"id":"wAgzUb3uxpLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word2vec\n","\n","Da documentação do Word2Vec temos:\n","\n","> In case you missed the buzz, Word2Vec is a widely used algorithm based on neural networks, commonly referred to as “deep learning” (though word2vec itself is rather shallow). Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word.\n","\n","> Word2Vec is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, strong and powerful would be close together and strong and Paris would be relatively far.\n","\n","Nesse contexto, podemos observar que o Word2Vec utiliza redes neurais rasas (shallow neural networks), sendo que no Word2Vec temos duas configurações:\n","\n","- Skip-gram\n","- CBOW (Continuous Bag-of-Words)\n","\n","\n","<img src=\"https://leimao.github.io/images/article/2019-08-23-Word2Vec-Classic/word2vec.png\">\n","\n","\n","No caso do CBOW, tentamos prever a palavra $w_{(t)}$ com base nas palavras da sua vizinhança (a palavra do meio).\n","\n","No caso do skip-gram, a partir da palavra $w_{(t)}$ tentamos prever as palavras na sua vizinhança.\n","\n","A camada do meio (conhecida como hidden-layer) é a camada que iremos utilizar, e ela forma o tamanho de vetor que desejamos ter (veremos melhor a seguir).\n","\n","\n","\n","\n","Mas o que significa isso? Conseguimos extrair informações importantes do contexto que a palavra se encontra e agrupar palavras de acordo com esse contexto!\n","\n","No caso de skip-grams\n","<img src=\"https://i.stack.imgur.com/fKkRF.png\">\n","\n","\n","E para o CBOW\n","\n","<img src=\"https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-1024x538.png\">\n","\n","Para saber mais:\n","\n","https://www.youtube.com/watch?v=wvsE8jm1GzE\n","\n","**Usar word2vec 10k, perplexity 25, lr, 10, procurar Austin, e one, drinks**\n","\n","http://projector.tensorflow.org/\n","\n","https://jalammar.github.io/illustrated-word2vec/"],"metadata":{"id":"eepKUX9ZsvHh"}},{"cell_type":"code","source":["class PreProcessPhrase:\n","\n","  def remove_acentuacao(self, phrase: str, debug: bool = False) -> str:\n","    # Utilizando a biblioteca unidecode para remover acentuação de texto\n","    phrase_fmt = unidecode(phrase)\n","    if debug:\n","      print('`remove_acentuacao`: Frase original', phrase)\n","      print('`remove_acentuacao`: Frase formatada', phrase_fmt)\n","\n","    # Retornando a frase formatada\n","    return phrase_fmt\n","\n","\n","  def remove_digits(self, phrase: str, debug: bool = False) -> str:\n","    # utilizando expressões regulares para remoção de digitos\n","    phrase_no_digits = re.sub(r'\\d', '', phrase)\n","\n","    # Se quisermos ligar o debug, mostre a frase original e transformada\n","    if debug:\n","      print('`remove_digits`: Texto original:', phrase)\n","      print('`remove_digits`: Texto sem digitos:', phrase_no_digits)\n","\n","    # Retornando a frase sem digitos\n","    return phrase_no_digits\n","\n","  def remove_special_char(self, phrase: str, debug: bool = False) -> str:\n","    # utilizando expressões regulares para remoção de caracteres especiais\n","    phrase_no_special_char = re.sub(r'[^a-zA-Z0-9]+', ' ', phrase)\n","\n","    # Se quisermos ligar o debug, mostre a frase original e transformada\n","    if debug:\n","      print('`remove_special_char`: Texto original:', phrase)\n","      print('`remove_special_char`: Texto sem caracteres especiais:', phrase_no_special_char)\n","\n","    # Retornando a frase sem digitos\n","    return phrase_no_special_char\n","\n","  def word_lower(self, word: str, debug: bool = False) -> str:\n","    try:\n","      # Formatando a palavra em caixa baixa\n","      word_fmt = word.lower()\n","\n","      # Se o debug for True, iremos imprimir a palavra original e transformada\n","      if debug:\n","        print('`word_lower`: Palavra Original:', word)\n","        print('`word_lower`: Palavra transformada:', word_fmt)\n","\n","    except:\n","      # Caso a palavra não seja uma string levante um erro (TypeError) informando qual o tipo da palavra passada\n","      raise TypeError(f'Esperava uma `word` no tipo str, foi passado uma {type(word)}')\n","\n","    # Retornando a palavra formatada\n","    return word_fmt\n","\n","  def remove_stopwords(self, words: List[str], debug=False) -> List[str]:\n","    # Carregando as stopwords (inglês)\n","    stopwords_en = stopwords.words('english')\n","    # Criando uma váriavel que irá armazenar elementos limpos, que não estejam dentro das stopwords\n","    clean_words = []\n","\n","    # Percorrendo cada palavra da nossa lista de palavras\n","    for word in words:\n","      # Verificando se a palavra não está presente das stopwords\n","      if word not in stopwords_en:\n","        # Se a palavra não é uma stopword adicionamos elas a váriavel clean_words\n","        clean_words.append(word)\n","      else:\n","        # Caso a palavra seja uma stopword e estamos no modo de debug (debug=True)\n","        if debug:\n","          # Imprimimos qual a palavra da lista words é uma stopword\n","          print(f'`remove_stopwords`: A palavra {word} está presente nas stopwords')\n","    return clean_words\n","\n","  def tokenizer(self, phrase: str, debug: bool) -> List[str]:\n","    words = word_tokenize(phrase)\n","    if debug:\n","        print('`tokenizer`: Frase original:', phrase)\n","        print('`tokenizer`: tokens:', words)\n","    return words\n","\n","  def stemmer(self, words: List[str], debug: bool = False) -> List[str]:\n","    # Inicializando o Porter Stemmer (inglês)\n","    stemmer = PorterStemmer()\n","    # Criando uma lista vazia para armazenar as palavras stem\n","    stem_words = []\n","    for word in words:\n","      # Pegando o stem de cada palavra\n","      s_word = stemmer.stem(word)\n","      # Adicionando essa palavra modificada a lista stem_words\n","      stem_words.append(s_word)\n","\n","    if debug:\n","        print('`stemmer`: Tokens originais:', words)\n","        print('`stemmer`: Tokens transformadods:', stem_words)\n","    return stem_words\n","\n","  def lemmatizer(self, words: List[str], debug: bool = False) -> List[str]:\n","    # Inicializando o WordNetLemmatizer (inglês)\n","    lemmatizer = WordNetLemmatizer()\n","    # Criando uma lista vazia para armazenar as palavras lemma\n","    lemm_words = []\n","    for word in words:\n","      # Pegando o lemma de cada palavra\n","      l_words = lemmatizer.lemmatize(word, pos='v')\n","      # Adicionando essa palavra modificada (lemma) a lista lemm_words\n","      lemm_words.append(l_words)\n","\n","    if debug:\n","        print('`stemmer`: Tokens originais:', words)\n","        print('`stemmer`: Tokens transformadods:', lemm_words)\n","    return lemm_words\n","\n","\n","  def pipeline(self, phrase: str, methods: List[str], debug: bool = False):\n","    switcher = {\n","        'remove_acentuacao': self.remove_acentuacao,\n","        'remove_digits': self.remove_digits,\n","        'remove_special_char': self.remove_special_char,\n","        'word_lower': self.word_lower,\n","        'remove_stopwords': self.remove_stopwords,\n","        'tokenizer': self.tokenizer,\n","        'stemmer': self.stemmer,\n","        'lemmatizer': self.lemmatizer\n","    }\n","    for method in methods:\n","      # remove_stopwords\n","      if method == 'remove_stopwords':\n","        phrase = switcher[method](phrase, debug=debug)\n","        \n","      else:\n","        phrase = switcher[method](phrase, debug=debug)\n","\n","    return phrase\n","\n"],"metadata":{"id":"B_IaPfkBtGrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WBUXIbQpxpIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lSUkCe1mxpGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textos = \\\n","[\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\",\n","\"Forest fire near La Ronge Sask. Canada\",\n","\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n","\"13,000 people receive #wildfires evacuation orders in California\",\n","\"Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school\"]"],"metadata":{"id":"ptdnLShVxpDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicando o preprocessamento de dados\n","# Instânciando o preprocessador de frases\n","preprocess = PreProcessPhrase()\n","# Definindo os passos da nossa pipeline\n","pipeline = [\n","    'remove_digits',\n","    'remove_special_char',\n","    'word_lower',\n","    'tokenizer',\n","    'remove_stopwords',\n","    'lemmatizer'\n","]\n","# Aplicando a pipeline de preprocessamento para cada documento (linha)\n","textos_processados = [' '.join(preprocess.pipeline(frase,methods=pipeline)) for frase in textos]"],"metadata":{"id":"6VUR0CopxpA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0POc13AztcRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lj9nQcOVtfsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x3WwUvuOtfps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uySkB5PMtfnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Podemos aplicar o CountVectorizer em novas frases\n","frase = \"alaska resident asked shelter place notified officer evacuation shelter place order expected\"\n"],"metadata":{"id":"L_zYebLKuJsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ele realiza a contagem de uma dada palavra na frase\n","# portanto se a frase tiver múltiplas ocorrências percebemos\n","# o aumento da contagem na coluna de feature\n","frase = \"resident asked shelter shelter shelter shelter shelter place notified officer alaska evacuation shelter place order expected\"\n"],"metadata":{"id":"-tRG2YefuSc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como mencionado anteriormente os documentos são complexos, e portanto pode aumentar muito a quantidade de features, principalmente pelo contexto (n-gramas).\n","\n","Para resolver esse problema, iremos utilizar o Word2Vec"],"metadata":{"id":"DNHn5UGtujn1"}},{"cell_type":"code","source":[],"metadata":{"id":"wXWCNkCZugad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xPZ4AAYyuyIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a5Ssia3qu0p3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wVb0FK_7vF54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZDpsijfdv5dC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gr8V105Wv-tE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EtAxzBOYwJBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O4FyGs42wfUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"awqKqpGkwhBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a4IRnxWOxCLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2gzZixzYxfmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zrsCq_a_xkqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EVP_0UA2xqKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hFfG0Uw8xvzv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No exemplo anterior o modelo Word2Vec foi treinado com poucos documentos.\n","\n","O Word2Vec fica interessante quando temos uma grande quantidade de documentos.\n","\n","Como a seguir:"],"metadata":{"id":"Z4n5eDMDyD4D"}},{"cell_type":"code","source":[],"metadata":{"id":"7Soc_4g0x_l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2af4l_y3yTWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7XUEJcXUyZdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zfVJdyGhykQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3DpRrAJ-ysGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uWzJkDFVywPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hrfkTTy3zaeq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Carregando\n","tweets = pd.read_csv('./9.3.tweets.csv', index_col=0)"],"metadata":{"id":"_2oai917zoi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicando o preprocessamento de dados\n","# Instânciando o preprocessador de frases\n","preprocess = PreProcessPhrase()\n","# Definindo os passos da nossa pipeline\n","pipeline = [\n","    'remove_digits',\n","    'remove_special_char',\n","    'word_lower',\n","    'tokenizer',\n","    'remove_stopwords',\n","    'stemmer'\n","]\n","# Aplicando a pipeline de preprocessamento para cada documento (linha)\n","tweets[\"filtered_words\"] = tweets['text'].apply(preprocess.pipeline, methods=pipeline)\n","\n","# Normalmente depois do processamento juntamos as palavras novamente em uma só string\n","tweets['join_words'] = tweets['filtered_words'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"3gHoWLjRHfBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definindo o target e as Features\n","# Para o Word2Vec utilizamos os tokens!\n","X = tweets['filtered_words']\n","y = tweets['target']"],"metadata":{"id":"ytazZpQUHe-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importando o `train_test_split` para separar os dados de treino e teste\n","from sklearn.model_selection import train_test_split\n","\n","# Separando o conjunto de dados em treino e teste (30% para teste)\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                     y, \n","                                                     test_size = 0.3, \n","                                                     random_state = 42)"],"metadata":{"id":"q_mHt5f6He79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6YzPA9Q0He29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-6hv25naI4WT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kWhszDYtJL7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2teGqtOmJ4PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uYbFPKqkKh5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1SyQ18F3KhoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ECXuLg2BMCC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Drops**\n","\n","Vamos juntar os passos acima em uma única função!\n","\n","A entrada é o documento (`doc`), o modelo ('Word2Vec`), método, e o debug.\n","\n","Preenchar o código abaixo para viabilizar a transformação de uma lista de tokens (List[str]) para números (vetor).\n","\n","O método será a média (como visto acima), ou a soma."],"metadata":{"id":"bYltYOrLI092"}},{"cell_type":"code","source":["def doc2vec(phrase: List[str], model: Word2Vec, method: str, debug: bool = False) -> np.ndarray:\n","  \n","  # Se o metodo passado for invalido gere um erro\n","  if method not in ['soma', 'media']:\n","      raise KeyError(f'Metodo não implementado {method}, escolha entre \"media\" e \"soma\"')\n","\n","  # Inicializando uma lista vazia para armazenar o resultado\n","  result_vec = []\n","  for token in phrase:\n","    # Verifique se o token está presente no vocabulário do model\n","\n","    if token in ...:  # PREENCHA AQUI\n","      # Se estiver presente, pegue o vetor correpondente da palavra\n","\n","      token_array = model...  # PREENCHA AQUI\n","\n","      result_vec.append(token_array)\n","\n","  # Verificando se a lista não é vazia\n","  # Caso ela não seja, ou seja pelo menos uma palavra está presente no vocabulário\n","  if result_vec:\n","    result_vec = np.asarray(result_vec)\n","    if method == 'media':\n","      result_vec =  ... # PREENCHA AQUI\n","    elif method == 'soma':\n","      result_vec = ... # PREENCHA AQUI\n","\n","  # Caso nenhuma palavra do doc esteja presente no vocabulário\n","  else:\n","    # Criando um vetor com zeros!\n","    result_vec = np.zeros(model.vector_size)\n","\n","  if debug:\n","    print(f'Frase: {phrase}')\n","    print(f'Vetor: {result_vec}')\n","  return result_vec"],"metadata":{"id":"WuIVHnI7I0br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testando a media \n","doc_vec1 = doc2vec(doc1, model, method='media', debug=True)"],"metadata":{"id":"0C2WkyVbI0ZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testando a soma \n","doc_vec2 = doc2vec(doc2, model, method='soma', debug=True)"],"metadata":{"id":"Z6LzazOiI0Wm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Passando um documento com uma palavra não existente no vocabulário\n","# Retorna um array de zeros de mesmo tamanho (500)\n","invalido_vec = doc2vec(['cachorrinha'], model, method='soma', debug=True)"],"metadata":{"id":"S2aI6g8WI0T2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Passando um método incorreto gera um erro\n","invalido_vec = doc2vec(['cachorrinha'], model, method='a', debug=True)"],"metadata":{"id":"JNQ5ta-wHe0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vamos transformar os dados de treino e teste de tokens para features númericas\n","X_train"],"metadata":{"id":"lxwkgEcPOvEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LSFsachLOjpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AKEcg9tyPsu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_seed = 42\n","# Importando os modelos de ensemble (Boosting e bagging)\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","# Importanto o modelo de regressão logística\n","from sklearn.linear_model import LogisticRegression\n","\n","# Importandos as métricas\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n","\n","\n","# Criando a nossa classe base (que contêm métodos que serão herdados pelas classes filhas)\n","class BaseModel:\n","  # Adicionando um nome de modelo (acessado por self.model_name)\n","  model_name = None\n","  # Adicionando um modelo base\n","  model = None\n","\n","  # Definindo um padrão (não necessário mas interessante as vezes)\n","  def fit(self):\n","    pass\n","  # Definindo um padrão (não necessário mas interessante as vezes)\n","  def predict(self):\n","    pass\n","  # Definindo um padrão (não necessário mas interessante as vezes)\n","  def predict_proba(self):\n","    pass\n","\n","  # Criando nossa função de avaliação do modelo\n","  # Recebe os argumentos\n","  # X: features\n","  # y_true: valores targets reais\n","  def evaluate(self, X, y_true):\n","    # Fazendo a predição utilizando as features de entrada\n","    y_predict = self.model.predict(X)\n","    # Calculando a curva ROC\n","    fpr, tpr, thresholds = roc_curve(y_true, self.model.predict_proba(X)[:,1])\n","    # Calculando o ROC-AUC\n","    auc = roc_auc_score(y_true, y_predict)\n","    # Calculando a acurácia\n","    accuracy = accuracy_score(y_true, y_predict)\n","    # Calculando a precisão\n","    precision = precision_score(y_true, y_predict, average='weighted')\n","    # Calculando a revocação\n","    recall = recall_score(y_true, y_predict, average='weighted')\n","\n","    # Calculando o F1-score\n","    f1 = f1_score(y_true, y_predict, average='weighted')\n","    # Inicializando o plot do gráfico de ROC\n","    # Adicionando a legenda sendo o nome do modelo e a AUC\n","    plt.plot(fpr, tpr, label=f'{self.model_name} ROC (AUC = {auc:.2f})')\n","    \n","    # Imprimindo dados do modelo:\n","    # Nome do modelo\n","    print(f\"Model      : {self.model_name}\")\n","    # A sua acurácia\n","    print(f\"Accuracy   : {accuracy:.4f}\")\n","    # A sua precisão\n","    print(f\"Precision  : {precision:.4f}\")\n","    # A sua revocação\n","    print(f\"Recall     : {recall:.4f}\")\n","    # O seu F1-score\n","    print(f\"F1 - Score : {f1:.4f}\")\n","    # A sua ROC-AUC\n","    print(f\"ROC - AUC  : {auc:.4f}\")\n","    # Imprimindo um divisor (para facilitar a visualização)\n","    print(\"======================\")\n","\n","    # Salvando os dados do modelo em um dicionário\n","    results = {\n","        \"model\": self.model_name,\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1-score\": f1,\n","        \"roc-auc\": auc\n","    }\n","    # Retornando o dicionário contendo os seus dados\n","    return results\n","\n","# Construindo uma classe de RandomForest que herde o modelo base\n","class MoviesRandomForest(BaseModel):\n","  # Função de inicialização\n","  def __init__(self, random_seed: int = 42, debug: bool = False):\n","    self.model_name = 'Random Forest'\n","    self.model = RandomForestClassifier(random_state = random_seed)\n","    self.debug = debug\n","\n","  def fit(self, X, y):\n","    if self.debug:\n","      print(f'Realizando o fit do modelo {self.model_name}')\n","    self.model.fit(X, y)\n","  def predict(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict do modelo {self.model_name}')\n","    return self.model.predict(X)\n","  def predict_proba(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict_proba do modelo {self.model_name}')\n","    return self.model.predict_proba(X)\n","\n","\n","class MoviesLogisticRegression(BaseModel):\n","# Função de inicialização\n","  def __init__(self, random_seed: int = 42, debug: bool = False):\n","    self.model_name = 'Logistic Regression'\n","    self.model =  LogisticRegression(random_state = random_seed, \n","                                  solver = 'lbfgs')\n","    self.debug = debug\n","\n","\n","  def fit(self, X, y):\n","    if self.debug:\n","      print(f'Realizando o fit do modelo {self.model_name}')\n","    self.model.fit(X, y)\n","  def predict(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict do modelo {self.model_name}')\n","    return self.model.predict(X)\n","  def predict_proba(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict_proba do modelo {self.model_name}')\n","    return self.model.predict_proba(X)\n","\n","class MoviesAdaBoost(BaseModel):\n","# Função de inicialização\n","  def __init__(self, random_seed: int = 42, debug: bool = False):\n","    self.model_name = 'Ada Boost'\n","    self.model = AdaBoostClassifier(random_state = random_seed)\n","    self.debug = debug\n","\n","\n","  def fit(self, X, y):\n","    if self.debug:\n","      print(f'Realizando o fit do modelo {self.model_name}')\n","    self.model.fit(X, y)\n","  def predict(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict do modelo {self.model_name}')\n","    return self.model.predict(X)\n","  def predict_proba(self, X):\n","    if self.debug:\n","      print(f'Realizando o predict_proba do modelo {self.model_name}')\n","    return self.model.predict_proba(X)\n","\n","def model_test_pipeline(models: List[str], X_train, X_test, y_train, y_test, debug: bool = False):\n","  model_switcher = {\n","      \"MoviesRandomForest\": MoviesRandomForest,\n","      \"MoviesLogisticRegression\": MoviesLogisticRegression,\n","      \"MoviesAdaBoost\": MoviesAdaBoost\n","  }\n","  final_results = []\n","  for model in models:\n","    selected_model = model_switcher[model](debug=debug)\n","    selected_model.fit(X_train, y_train)\n","    model_results = selected_model.evaluate(X_test, y_test)\n","    final_results.append(model_results)\n","  # Adicionando estilo ao plot\n","  plt.plot([0, 1], [0, 1], 'r--')\n","  plt.xlim(0, 1)\n","  plt.ylim(0, 1.05)\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('ROC-AUC curve')\n","  plt.legend(loc='lower right')\n","  plt.show()\n","\n","  # Transformando os resultados em uma tabela\n","  results_df = pd.DataFrame(final_results)\n","  return results_df\n"],"metadata":{"id":"NUtkcuSYO4qX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OgiVzbtFPNsL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"djcOCQZuPX2l"},"execution_count":null,"outputs":[]}]}