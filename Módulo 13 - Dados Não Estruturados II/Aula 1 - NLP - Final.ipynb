{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HFkxmHfd_R"
   },
   "source": [
    "## Aula 1 - Processamento de Linguagem Natural\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "1) Dados Estruturados e Não Estruturados.  \n",
    "2) Introdução a NLP.  \n",
    "3) Processamento de Textos.  \n",
    "4) Exercícios.  \n",
    "\n",
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgvh29EJh6UK"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferença enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **não estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** São dados que seguem uma estrutura mais rígida com um padrão fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados Não estruturados:** Como já diz o nome, são dados que não tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: áudios, vídeos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whQI1eePiJwx"
   },
   "source": [
    "### Introdução ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwjo0xx9h-Pi"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, é a abordagem onde trabalhamos com **dados não estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos é extrair de informação e teor linguístico das nossas bases de textos e converter isso de uma forma númerica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplicações de NLP como:\n",
    "- Análise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e Não Spams;\n",
    "- Identificação de textos a partir de construções linguísticas (descobrir se um texto foi escrito ou não por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortográficos;\n",
    "- Classificação de textos de acordo com o conteúdo do texto (Esportes, Política, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de referência para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsI5e-Sjn_U1"
   },
   "source": [
    "**Drops** \n",
    "\n",
    "Procure outras aplicações de NLP, pode ser na área que trabalha!\n",
    "\n",
    "Por exemplo, na área financeira\n",
    "____\n",
    "R:\n",
    "- [Cálculo de Indicadores Financeiros a partir de relatórios empresariais](https://repositorio.ufsc.br/bitstream/handle/123456789/228147/TCC.pdf?sequence=1&isAllowed=y)\n",
    "- [Auditoria Financeira](https://www.linkedin.com/pulse/5-aplica%C3%A7%C3%B5es-de-processamento-linguagem-natural-em-servi%C3%A7os-rocha/?originalSubdomain=pt)\n",
    "- [Identificação do elemento sentencioso](https://morethandigital.info/pt-pt/nlp-explained-o-que-e-processamento-de-linguagem-natural/#2_Classificacao_de_diferentes_sequencias)\n",
    "- [mercado de ações](https://towardsdatascience.com/nlp-in-the-stock-market-8760d062eb92)\n",
    "- [Chatbot whatsapp**](https://meudroz.com/category/whatsapp/)\n",
    "- [classificação de eventos na área de saúde](https://www.sciencedirect.com/science/article/abs/pii/S1386505619302370)\n",
    "\n",
    "** Ver twilio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjXHV5QOjWDc"
   },
   "source": [
    "### Processamento de Textos\n",
    "\n",
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conteúdo de mais relevantes para a nossa análise. Existem alguns processos importantes para trabalhar com os textos (não necessariamente você precisa aplicar todos os procesos)\n",
    "\n",
    "- Remoção de Stopwords;\n",
    "- Limpeza de Textos;\n",
    "- Tokenização;\n",
    "- Normalização do Texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9je2oXCjjrwc"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que aparecem com uma frequência muito alta nos textos, mas que não trazem um teor de conteúdo relevante para o nosso modelo. Vamos entender isso na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1661208599050,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "3IR-pN2DjwXJ",
    "outputId": "a7ea4e32-861a-48c6-bec2-deb347679a2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importanto o NLTK\n",
    "import nltk\n",
    "# Importando o stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Fazendo o download das stopwords do nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('rslp')  # Stemmer para português"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR2RHfCOn83i"
   },
   "source": [
    "Baixada as Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em inglês já identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1661208640435,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "-t93Bq2FncDC"
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1661208752242,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "LllQ0D8sol9G",
    "outputId": "422daa7d-105d-4c4a-a7ef-c67c95f26130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661208761352,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "ClhJwZvOoqI-",
    "outputId": "d4cb4cb8-f480-459a-fb09-5a5ccef7e775"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1661208769327,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "3OEsva3po-FN",
    "outputId": "63f0940d-e230-47b2-f8ed-7cdec8e4dd69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1661208809122,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "vcQKI7ZdolbV"
   },
   "outputs": [],
   "source": [
    "stopwords_port = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661208812893,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "-16r_klQolCC",
    "outputId": "9f7238d3-a3a1-4c7a-c315-88e58e87e8c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1661208844073,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "Y6gP4YowoyYk",
    "outputId": "01c744f8-8877-474a-b429-875f6b04e14b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661208871215,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "TyHAf5Uko0ZQ",
    "outputId": "1b1cc1c7-6658-4f43-c563-a53e0093f965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9Dpg-8ApIxa"
   },
   "source": [
    "Vamos agora aplicar a remoção de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvrcEJsQo_tf"
   },
   "outputs": [],
   "source": [
    "example = [\"my\", \"house\", \"is\", \"black\", \"and\", \"white\", \"but\", \"isn't\", \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1661208954344,
     "user": {
      "displayName": "Gilberto Kaihami",
      "userId": "07651446941847976228"
     },
     "user_tz": 180
    },
    "id": "d8EHLUJqxOiy",
    "outputId": "7e5b40f8-9cc9-4e7b-87ec-597761293a02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDF1D4zVpNTK"
   },
   "source": [
    "**Drops**\n",
    "\n",
    "Crie uma função que remova da lista `example` elementos que estejam presentes na lista `stopwords_en`.\n",
    "\n",
    "Nessa função deve ter três parâmetros:\n",
    "- words\n",
    "- stopwords\n",
    "- debug\n",
    "\n",
    "A `words` é uma lista de palavras, como a variável `example` criada acima (List[str]).\n",
    "\n",
    "A `stopwords` é uma lista de palavras de stopwords, como a variável `stopwords_en` criada acima (List[str]).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, identificando quais palavras/elementos foram removidos durante a limpeza dos dados. Esse parâmetro é um boolean (True/False), se verdadeiro iremos imprimir quais palavras foram retiradas.\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUaavlQhtf8g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dApkKy6sJFp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAg9BYDItTpx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVe-4PuSpMBQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLUUxouBt20M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqzUgiFkrNSJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuslDstSsCpJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSmVHk8MxHSS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnYz481KOjkM"
   },
   "source": [
    "### Limpeza do Texto\n",
    "\n",
    "Existem alguns cuidados com relação a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos são:<br><br>\n",
    "- Transformar todas as palavras para MAIÚSCULAS ou minúsculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover dígitos (quando não forem relevantes);\n",
    "- Remover acentuação (caso típico de quando trabalhamos com textos em Português);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9xYl3yhOswd"
   },
   "source": [
    "### Converter entre MAIÚSCULA e minúscula\n",
    "\n",
    "**Drops**\n",
    "\n",
    "Crie uma função que receba uma palavra e normalizando-as em caixa alta ou baixa.\n",
    "\n",
    "Nessa função deve ter três parâmetros:\n",
    "- word\n",
    "- debug\n",
    "\n",
    "A `word` palavra, como por exemplo `Relógio` (str).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a palavra antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg4DogilOgcG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4kz-e3tSjuO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU99j1FMQevr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msC_BxNLR1NX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSt1MM8IU8Y1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQH1sefETv53"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY70APJHWcne"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHQuk4TRU-Wd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYRTjdWiTKWu"
   },
   "source": [
    "### Remoção de dígitos, caracteres especiais e qualquer outro item que não queremos no texto\n",
    "\n",
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE4HC6guTNGR"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2D1Od-TVVx"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a função *re.sub*, para substituir os elementos que não queremos nos nossos textos:\n",
    "\n",
    "**Procure por re.sub**  \n",
    "https://docs.python.org/3/library/re.html\n",
    "\n",
    "\n",
    "#### Removendo digitos\n",
    "**Drops**\n",
    "\n",
    "Crie uma função que receba uma frase e remova os digitos ([0-9]).  \n",
    "Nessa função deve ter dois parâmetros:\n",
    "- phrase\n",
    "- debug\n",
    "\n",
    "A `phrase`, é uma frase como por exemplo `'Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers'` (str).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a frase antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código.\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ2RZpGGTSSn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIdEff7aWkw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K7Ns8chXW2I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoklF4g7XvM-"
   },
   "outputs": [],
   "source": [
    "frase = 'Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKrLqy3bX1k0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecSQLsU9X3SC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HcZ7XmNYAM_"
   },
   "source": [
    "#### Removendo caracteres especiais\n",
    "**Drops**\n",
    "\n",
    "Crie uma função que receba uma frase e remova os caracteres especiais (p.ex, ã,õ, ê, @, #, etc).  \n",
    "Nessa função deve ter dois parâmetros:\n",
    "- phrase\n",
    "- debug\n",
    "\n",
    "A `phrase`, é uma frase como por exemplo `'Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers'` (str).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a frase antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código.\n",
    "\n",
    "Obs: Utilize o site https://regex101.com/ para criar o regex e o re.sub para substituir.\n",
    "\n",
    "Obs2: Procure por alfanumérico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYiu-XQDYmfd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9Urf2soY948"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVdBXrGVZLHB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMxkvL3FZO08"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwc6j-dzZsXP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqfqkjuabuH"
   },
   "source": [
    "Utilizem a documentação para descobrir mais códigos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplicações do RegEx: [clique aqui](https://amitness.com/regex/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeoTPv_4agE2"
   },
   "source": [
    "Hoje os emojis fazem parte da comunicação via mensagens, por isso iremos ver como utilizar frases contento emojis e trata-los de forma adequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Twyme__7afbF"
   },
   "outputs": [],
   "source": [
    "# Iremos utilizar a biblioteca emoji\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCZwZhJLaTnj"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb4EGxBSatqv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcRH6cAZbONN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9HCV1QKbjXG"
   },
   "source": [
    "Para ver a lista completa:\n",
    "\n",
    "https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "\n",
    "Link biblioteca: https://github.com/carpedm20/emoji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7ZrPwFqb7KC"
   },
   "source": [
    "#### Convertendo emojis para texto\n",
    "**Drops**\n",
    "\n",
    "Crie uma função que receba uma frase e converta os emojis para texto\n",
    "Nessa função deve ter dois parâmetros:\n",
    "- phrase\n",
    "- debug\n",
    "\n",
    "A `phrase`, é uma frase como por exemplo `Python is 👍` (str).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a frase antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Gf3OP6WbrFU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orI4UsErcU4h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1RYVu_5dBHH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caQR5XL1dCzs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZap5orRdLUE"
   },
   "source": [
    "Agora que vimos diversas funções e tratamentos, podemos utilizar o regex para validação de e-mail, por exemplo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftT89PXndWFh"
   },
   "source": [
    "#### Crie uma função que receba uma string e valide se é possívelmente um e-mail\n",
    "**Drops**\n",
    "\n",
    "Crie uma função que receba um email e verifique se é um formato válido de e-mail\n",
    "- email\n",
    "- debug\n",
    "\n",
    "A `email`, é uma frase como por exemplo `myemail@gmail.com` (str).\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a frase antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CilxbjvAdSxl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtubRPQHey_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMQ9SchZe1Lf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlxzMtefggjm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMf6iNGAf89I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae80wZOIgtyP"
   },
   "source": [
    "Aplicação de NLP para validação de campos.  \n",
    "- Muito útil para evitar ataques de baixa qualidade \n",
    "- Evitar cadastro de pessoas inválidas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoYA1wYg6Yj"
   },
   "source": [
    "Podemos utilizar NLP para identificar frases que contenham uma palavra chave!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMWmNH37gQCS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWTvxMBQhB-G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5f4aJfl7QCC"
   },
   "source": [
    "### Remoção de Acentuação\n",
    "Para a remoção de acentuação, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7hpJwTa7SIY"
   },
   "outputs": [],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o código abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLSKg7A7TS5"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFxZKdkJ7VHI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinjRSkG7ZPB"
   },
   "source": [
    "**Drops**\n",
    "\n",
    "Que remova a acentuação das palavras de uma frase\n",
    "- phrase\n",
    "- debug\n",
    "\n",
    "A `phrase`, é uma frase como por exemplo 'João Sebastião Alvará Vovô Linguiça expressão'.\n",
    "\n",
    "O parâmetro `debug` irá auxiliar no debug, podendo imprimir a frase antes e após a transformação\n",
    "\n",
    "**Extra:**\n",
    "\n",
    "Comumente utilizamos TDD (test driven development).\n",
    "\n",
    "Portanto crie um teste para essa função para garantir a qualidade de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD3UOkNu7WqD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQg499Zr8auG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtMjFbA-8pHa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbHMYcx38qek"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhV4Z2xB89HI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcoeEmpC9Kbc"
   },
   "source": [
    "## Tokenização\n",
    "\n",
    "Tokenização é um processo onde transformamos um texto de uma string única em fragmentos desse texto na forma de *tokens*, que nada mais são do que as próprias palavras! Para isso, vamos utilizar a função *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucLVo_6j9L9x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOwuFw8Q9O2s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcgxYEK19PjP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qed6o8G9Vib"
   },
   "source": [
    "## Normalização de Textos\n",
    "\n",
    "**Normalização de Textos (Text Normalization)** é o procedimento que consiste em **padronizar** o texto, de modo a evitar que variações tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou então eliminar conjugação de verbos. Outras componentes comuns da normalização são a de eliminar palavras que não agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de ações de Text Normalization que podem ser aplicadas no pré-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redução de tokens à sua raiz invariante através da **remoção de prefixos ou sufixos**. Baseado em heurística<br>\n",
    "**Lemmatization** - Redução de tokens à sua raiz invariante através da **análise linguística do token**. Baseado em dicionário léxico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieG5enJ79aUg"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA-tVw5S9RGm"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TLZDxkF9l8r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSZhPhGP9cqF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf0ggw2I9qFx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7Ee5lvk9gK9"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "Obs: Não funciona para português, precisamos utilizar o Spacy (outra biblioteca para NLP), veremos mais adiante no curso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnQrtAl_9d_n"
   },
   "outputs": [],
   "source": [
    "# Importando o Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# https://www.nltk.org/howto/wordnet.html\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# NLTK 3.6.6 release: December 2021:\n",
    "# support OMW 1.4, use Multilingual Wordnet Data from OMW with newer Wordnet versions\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-bLgWtE_Xpc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaqVvMR-_cfF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4j3dnhJ_d1n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UihVRxqc_0Co"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxH2uhI4A8mq"
   },
   "source": [
    "## Pipeline de Processamento de Textos\n",
    "\n",
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma útil e organizada para isso é construirmos uma funçãi que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4qLqa46BBUi"
   },
   "source": [
    "**drops**\n",
    "\n",
    "Crie uma classe que seja capaz de:\n",
    "\n",
    "- Metodo para remover acentuação\n",
    "- Metodo de remover digitos\n",
    "- Metodo de remover caracteres especiais\n",
    "- Metodo de normalizar o texto em caixa baixa\n",
    "- Metodo para criar os tokens\n",
    "- Metodo para filtrar stopwords\n",
    "- Metodo para pegar o stemming\n",
    "- Metodo para pegar o lemma\n",
    "- Metodo de pipeline.\n",
    "\n",
    "Obs: Não iremos fazer os testes em classe, mas é um desafio interessante de ser realizado pós-aula para treinar TDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQSRgAlFAAnB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brOZjdj3Cp96"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsFClqwMHbvb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDWoIbbJHfKz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eq1dJuERHw7_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWOcLEWiH8yA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfFVnJPxIDvj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkU2HCMLILSn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhKiVPx5IiLf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PACJIBDJJ3O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJB5z7LpJTMB"
   },
   "source": [
    "Vamos agora já começar a práticar com os nossos dados de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8gBYe6iJPcL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF2sg0DYJRRK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Fe4sNUVJiKT"
   },
   "source": [
    "Nosso exemplo será uma Análise de Sentimento em Críticas de Filmes, onde vamos identificar se a crítica foi boa ou não:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVkn3PA9JfJ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBt2MOmoJlAK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGJi_cyTJmSP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egXSFU8wJokk"
   },
   "source": [
    "A nossa base de dados tem 50 mil linhas e levando em consideração que as críticas são sobre filmes diversos, a quantidade de palavras disponíveis nos textos será muito grande. Para economizar tempo de aula com processamneto dos textos e modelagem, iremos criar uma amostra com 10% da base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjNnxvhNJpHa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ5c0kNVJr62"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E39T4pD8Jttk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up0Jq5VjJxOE"
   },
   "source": [
    "Agora iremos aplicar o nosso processamento dos textos:\n",
    "\n",
    "**Drops**\n",
    "\n",
    "Aplique o pipeline acima da classe `PreProcesssPhrase()`\n",
    "```\n",
    "pipeline = [\n",
    "    'remove_digits',\n",
    "    'remove_special_char',\n",
    "    'word_lower',\n",
    "    'tokenizer',\n",
    "    'remove_stopwords',\n",
    "    'stemmer'\n",
    "]\n",
    "```\n",
    "\n",
    "Na base de dados movies, coluna `text`, atribua o resultado numa coluna chamada `filtered_words`\n",
    "\n",
    "Obs: pode ser utilizado o apply para tal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRq2rK6zJtq7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ2c4TInLOrq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BloPLguAJtoE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWQEv92ALVPk"
   },
   "source": [
    "## Corpus\n",
    "\n",
    "Com isso chegamos ao fim do pré-processamento, uma das etapas mais importantes de todo projeto de NLP!\n",
    "\n",
    "É importante ressaltar que a escolha das etapas de pré-processamento não é algo óbvio, dado que há muitas escolhas possíveis acerca do que se fazer para pré-processar os dados. Assim, o indicado é treinar diferentes modelos testando diferentes combinações das técnicas de pré-processamento, até que o melhor procedimento seja encontrado!\n",
    "\n",
    "**Nomenclatura**: o conjunto de mensagens (também conhecido como documentos) pré-processadas é chamado de **Corpus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugfwM9cOLYts"
   },
   "source": [
    "## Vocabulário\n",
    "\n",
    "O vocabulário do corpus nada mais é do que uma listagem das palavras individuais que aparecem no corpus. Para encontrar o vocabulário, basta contarmos a aparição de cada palavra isolada no corpus. Ao fim, teremos N palavras únicas que compõem nosso vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwwjo0QzJtlM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zi-hdDGVJtid"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaDYDJLkJtfc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4G1jkmlgLl5r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ0eJb_JLmUX"
   },
   "source": [
    "### Criando nossa nuvem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Z8aqO6eJtck"
   },
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3G_skT-JtU_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvelnSM4LrNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrGD0nDYLrJr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKO6pDTTMDWl"
   },
   "source": [
    "**Vocabulário de críticas positivas**\n",
    "\n",
    "Criando a wordcloud apenas de críticas positivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma7-FGpoLrDb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmoFx5dlML9X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiIS-XMEMO3W"
   },
   "source": [
    "**Vocabulário de críticas negativas**\n",
    "\n",
    "Criando a wordcloud apenas de críticas negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MJPp941ML6l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf66OnpYMVSL"
   },
   "source": [
    "## Exercícios\n",
    "\n",
    "**1)** Usando a base *spamraw.csv*, faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e imprima o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClHbN5ezMUti"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLfthU86MggL"
   },
   "source": [
    "**2)** Utilizando os dados de tweets vamos avaliar  tweets são de desastres ou não. Essa base é um dataset conhecido do Kaggle, onde vocês podem ter mais detalhes [clicando aqui](https://www.kaggle.com/c/nlp-getting-started/overview). Faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e print o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOfCHWPENHWc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 1 - NLP - Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
