{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 5 - Hipóteses mais complexas e regularização\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Hipóteses mais complexas\n",
    "- 2) Regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:19.400131Z",
     "start_time": "2022-02-04T21:13:16.569568Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## 1) Hipóteses mais complexas\n",
    "\n",
    "Muitas vezes, temos dados que simplesmente não se ajustam às hipóteses simples, lineares, que conhecemos até o momento.\n",
    "\n",
    "Quando isso acontece, é muito provável que soframos **underfitting**, pois uma forma funcional demasiadamente simples de uma hipótese pode não ser capaz de capturar o comportamento de uma função teórica $\\mathcal{F}$ mais complexa, conforme refletido pela amostra.\n",
    "\n",
    "Nestes casos, a solução é simples: basta escolhermos hipóteses mais complexas!\n",
    "\n",
    "Pra começar nosso estudo, vamos utilizar dados bem simples do próprio sklearn (submódulo [datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:21.039404Z",
     "start_time": "2022-02-04T21:13:19.400131Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 700, n_features = 1,\n",
    "                       noise = 35, tail_strength = 50,\n",
    "                       random_state = 42)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:21.275247Z",
     "start_time": "2022-02-04T21:13:21.042382Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples = 700, n_features = 1,\n",
    "                       noise = 35, tail_strength = 50,\n",
    "                       random_state = 42)\n",
    "\n",
    "y = y**2\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer uma regressão linear..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:26.177078Z",
     "start_time": "2022-02-04T21:13:21.279245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# como temos uma unica feature, dá pra plotar o modelo (hipótese final)\n",
    "print(\"\\nModelo treinado:\")\n",
    "\n",
    "x_plot = np.linspace(X.min(), X.max(), 1000)\n",
    "\n",
    "y_plot = reg_lin.intercept_ + reg_lin.coef_[0]*x_plot\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"red\")\n",
    "\n",
    "# dados\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"Métricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturalmente, temos métricas bem ruins, dada a escolha ruim de hipótese!\n",
    "\n",
    "Hipótese atual:\n",
    "\n",
    "$$f_{h, \\  \\vec{b}}(x) = b_0 + b_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer algo melhor: como nossos dados são aproximadamente quadráticos, faria sentido escolher uma **hipótese quadrática**, não é mesmo? Isto é,\n",
    "\n",
    "$$f_{h, \\  \\vec{b}}(x) = b_0 + b_1x + b_2x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E é aqui que entra um dos aspectos mais importantes de um modelo linear como a regressão linear: **o modelo é linear nos parâmetros, não necessariamente nas features!**\n",
    "\n",
    "Ou seja, o termo quadrado que incluímos **pode ser considerado como uma nova feature linear**. Para ver isso, basta definir $z \\equiv x^2$, que voltamos a ter uma hipótese linear, mas agora em duas variáveis:\n",
    "\n",
    "$$f_{h, \\  \\vec{b}}(x, z) = b_0 + b_1x + b_2z$$\n",
    "\n",
    "Ou seja, ainda temos uma regressão linear (múltipla, agora).\n",
    "\n",
    "E isso é verdade para **qualquer** combinação de features que possamos criar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, para criarmos um modelo quadrático para nossos dados, bastaria criarmos uma nova feature $z = x^2$, e passar apenas esta nova feature para o  modelo de regressão linear **simples**. Isso equivale a usar uma hipótese $$f_{h, \\  \\vec{b}}(z) = b_0 + b_1z = b_0 + b_1x^2$$\n",
    "\n",
    "Vejamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:26.395184Z",
     "start_time": "2022-02-04T21:13:26.179584Z"
    }
   },
   "outputs": [],
   "source": [
    "# isso a feature z = x^2\n",
    "# note: isso é um PRE PROCESSAMENTO DOS DADOS!!! nao to mexendo em NADA do estimador\n",
    "Z = X**2\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# note que aqui tamos usando o Z ao inves do X\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# como temos uma unica feature, dá pra plotar o modelo (hipótese final)\n",
    "print(\"\\nModelo treinado:\")\n",
    "\n",
    "x_plot = np.linspace(X.min(), X.max(), 1000)\n",
    "\n",
    "y_plot = reg_lin.intercept_ + reg_lin.coef_[0]*(x_plot**2)\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"red\")\n",
    "\n",
    "# dados\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"Métricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, um modelo beeem melhor!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se quisermos usar a hipótese quadrática mais completa, com ambos os termos linear e quadrático? (Isto é, $f_{h, \\  \\vec{b}}(x) = b_0 + b_1x + b_2x^2$)\n",
    "\n",
    "Bem simples: basta passarmos as duas features pro sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:26.441649Z",
     "start_time": "2022-02-04T21:13:26.395184Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X, columns=[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:26.855532Z",
     "start_time": "2022-02-04T21:13:26.443627Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:27.037135Z",
     "start_time": "2022-02-04T21:13:26.861529Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df[\"Z\"] = X_df[\"X\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:27.289095Z",
     "start_time": "2022-02-04T21:13:27.037135Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:27.938379Z",
     "start_time": "2022-02-04T21:13:27.294073Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# note que aqui tamos usando o X_df (com as duas variaveis) ao inves do X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# como temos uma unica feature, dá pra plotar o modelo (hipótese final)\n",
    "print(\"\\nModelo treinado:\")\n",
    "\n",
    "x_plot = np.linspace(X.min(), X.max(), 1000)\n",
    "\n",
    "y_plot = reg_lin.intercept_ + reg_lin.coef_[0]*(x_plot) + reg_lin.coef_[1]*(x_plot**2)\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"red\")\n",
    "\n",
    "# dados\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"Métricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No geral, dá pra ir aumentando a ordem dos polinomios criando features de ordem maior uma a uma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:27.954398Z",
     "start_time": "2022-02-04T21:13:27.941324Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df[\"A\"] = X_df[\"X\"]**3\n",
    "X_df[\"B\"] = X_df[\"X\"]**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:28.098033Z",
     "start_time": "2022-02-04T21:13:27.956399Z"
    }
   },
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:28.588168Z",
     "start_time": "2022-02-04T21:13:28.101033Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# note que aqui tamos usando o X_df (com as duas variaveis) ao inves do X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# como temos uma unica feature, dá pra plotar o modelo (hipótese final)\n",
    "print(\"\\nModelo treinado:\")\n",
    "\n",
    "x_plot = np.linspace(X.min(), X.max(), 1000)\n",
    "\n",
    "y_plot = (reg_lin.intercept_ + reg_lin.coef_[0]*(x_plot)\n",
    "          + reg_lin.coef_[1]*(x_plot**2) + reg_lin.coef_[2]*(x_plot**3)\n",
    "          + reg_lin.coef_[3]*(x_plot**4))\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"red\")\n",
    "\n",
    "# dados\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"Métricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O procedimento acima é bem manual. Pra nossa sorte, o sklearn existe, e uma de suas muitas ferramentas especiais para machine learning (no caso, pré-processamento) é o [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), que permite a criação de toda as combinações polinomiais de features automaticamente!\n",
    "\n",
    "O PolynomialFeatures é nosso primeiro exemplo de **transformer** do sklearn - um método cujo objetivo é aplicar alguma **transformação** aos dados. Veremos vários outros exemplos de transformers durante o curso.\n",
    "\n",
    "Em particular, todos os transformers se comportam como se fossem \"estimadores\", no sentido de que eles devem \n",
    "ser \"ajustados\" aos dados -- por isso, eles também têm o método `.fit()` -- que ajusta o transformer aos dados; além do método `.transform()`, que efetivamente transforma os dados. Existe também o `.fit_transform()`, que faz as duas coisas ao mesmo tempo -- mas vamos evitar de usá-lo, por motivos que ficarão claros no futuro próximo.\n",
    "\n",
    "Lembre-se de fitar o transformador sempre nos dados de treino, apenas! Neste caso, não faz muita diferença, mas, para nos acostumarmos a isso, vamos fazer aqui também!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:28.780063Z",
     "start_time": "2022-02-04T21:13:28.591167Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples = 700, n_features = 1,\n",
    "                       noise = 35, tail_strength = 50,\n",
    "                       random_state = 42)\n",
    "\n",
    "y = y**2\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:28.811688Z",
     "start_time": "2022-02-04T21:13:28.785063Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:28.906688Z",
     "start_time": "2022-02-04T21:13:28.815675Z"
    }
   },
   "outputs": [],
   "source": [
    "vars(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.011554Z",
     "start_time": "2022-02-04T21:13:28.910687Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.106524Z",
     "start_time": "2022-02-04T21:13:29.016564Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.n_output_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.233428Z",
     "start_time": "2022-02-04T21:13:29.111496Z"
    }
   },
   "outputs": [],
   "source": [
    "# das duas features que teremos depois da transformação\n",
    "# a primeira tem grau 1\n",
    "# a segunda tem grau 2\n",
    "\n",
    "pf.powers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.338099Z",
     "start_time": "2022-02-04T21:13:29.236426Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_transf = pf.transform(X_train)\n",
    "X_test_transf = pf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.430597Z",
     "start_time": "2022-02-04T21:13:29.338099Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.604260Z",
     "start_time": "2022-02-04T21:13:29.435594Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo numa unica célula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.958041Z",
     "start_time": "2022-02-04T21:13:29.613259Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# passo adicional: criando features polinomiais\n",
    "# pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)\n",
    "\n",
    "print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "# redefinindo as features de treino e de teste\n",
    "X_train = pf.transform(X_train)\n",
    "X_test = pf.transform(X_test)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# como temos uma unica feature, dá pra plotar o modelo (hipótese final)\n",
    "print(\"\\nModelo treinado:\")\n",
    "\n",
    "x_plot = np.linspace(X.min(), X.max(), 1000)\n",
    "\n",
    "# pra plotar a hipotese automaticamente\n",
    "# y = bo + b1*x^1 + b2*x^2 + .... + bn*x^n\n",
    "\n",
    "# y_plot = reg_lin.intercept_\n",
    "\n",
    "# for i in range(len(reg_lin.coef_)):\n",
    "    \n",
    "#     y_plot = y_plot + reg_lin.coef_[i]*(x_plot**(i+1))\n",
    "\n",
    "y_plot = reg_lin.intercept_\n",
    "\n",
    "for n, b_n in enumerate(reg_lin.coef_):\n",
    "    \n",
    "    y_plot = y_plot + b_n*(x_plot**(n+1))\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"red\")\n",
    "\n",
    "# dados\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"Métricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Então podemos pensar que quanto mais features melhor será o nosso modelo?\n",
    "\n",
    "**Maldição da dimensionalidade**\n",
    "\n",
    "Este fenômeno afirma que com um número fixo de amostras de treinamento, o poder preditivo médio (esperado) de um classificador ou regressor aumenta primeiro à medida que o número de dimensões ou características utilizadas aumenta, mas além de uma certa dimensionalidade, começa a deteriorar-se em vez de melhorar de forma constante. Este aumento na dimensionalidade do problema pode se refletir no overfitting de um modelo. Vamos ver isso claramente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:29.976545Z",
     "start_time": "2022-02-04T21:13:29.960921Z"
    }
   },
   "outputs": [],
   "source": [
    "# prototipo pra salvar resultados\n",
    "\n",
    "# resultados = {\"num_features\" : [1, 2, 3],\n",
    "#               \"mae_train\" : [425432, 454253324, 435645654676798],\n",
    "#               \"mae_test\" : [42424325, 5434234253, 435645654676798]}\n",
    "\n",
    "# pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototipo do que fizemos abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:30.101535Z",
     "start_time": "2022-02-04T21:13:29.976545Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados = {\"num_features\" : [],\n",
    "              \"mae_train\" : [],\n",
    "              \"mae_test\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:30.257847Z",
     "start_time": "2022-02-04T21:13:30.101535Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados[\"num_features\"].append(4)\n",
    "resultados[\"mae_train\"].append(5453435)\n",
    "resultados[\"mae_test\"].append(45345345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:30.398745Z",
     "start_time": "2022-02-04T21:13:30.257847Z"
    }
   },
   "outputs": [],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.100926Z",
     "start_time": "2022-02-04T21:13:30.401929Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dicionario de resultados do experimento\n",
    "resultados = {\"num_features\" : [],\n",
    "              \"mae_train\" : [],\n",
    "              \"mae_test\" : []}\n",
    "\n",
    "for grau in range(1, 16):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    pf = PolynomialFeatures(degree=grau, include_bias=False)\n",
    "\n",
    "    pf.fit(X_train)\n",
    "\n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "    # redefinindo as features de treino e de teste\n",
    "    X_train = pf.transform(X_train)\n",
    "    X_test = pf.transform(X_test)\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    reg_lin = LinearRegression()\n",
    "\n",
    "    reg_lin.fit(X_train, y_train)\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    # predições de treino\n",
    "    y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "    print(\"Métricas de treino:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "    # predições de teste\n",
    "    y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "    print(\"\\nMétricas de teste:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"#\"*80)\n",
    "    print()\n",
    "    \n",
    "    # ======================================\n",
    "    \n",
    "    resultados[\"num_features\"].append(pf.n_output_features_)\n",
    "    resultados[\"mae_train\"].append(mean_absolute_error(y_train, y_pred_train))\n",
    "    resultados[\"mae_test\"].append(mean_absolute_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.118816Z",
     "start_time": "2022-02-04T21:13:31.103173Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataframe de resultados do experimento\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.228287Z",
     "start_time": "2022-02-04T21:13:31.118816Z"
    }
   },
   "outputs": [],
   "source": [
    "df_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.338778Z",
     "start_time": "2022-02-04T21:13:31.229549Z"
    }
   },
   "outputs": [],
   "source": [
    "df_resultados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.640576Z",
     "start_time": "2022-02-04T21:13:31.341759Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Tradeoff viés-variância\")\n",
    "\n",
    "plt.plot(df_resultados[\"num_features\"], df_resultados[\"mae_train\"], label=\"MAE train\")\n",
    "plt.plot(df_resultados[\"num_features\"], df_resultados[\"mae_test\"], label=\"MAE test\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:31.793475Z",
     "start_time": "2022-02-04T21:13:31.643106Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Tradeoff viés-variância\")\n",
    "\n",
    "plt.plot(df_resultados[:11][\"num_features\"], df_resultados[:11][\"mae_train\"], label=\"MAE train\")\n",
    "plt.plot(df_resultados[:11][\"num_features\"], df_resultados[:11][\"mae_test\"], label=\"MAE test\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "_____________\n",
    "_____________\n",
    "\n",
    "Agora que já entendemos a técnica em um dataset bem simples, vamos voltar pra um dataset real!\n",
    "\n",
    "Vamos voltar pros dados da precificação de casas -- ali, o poly_features se mostrará ainda mais útil!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:32.074708Z",
     "start_time": "2022-02-04T21:13:31.793475Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"house_prices.csv\")\n",
    "\n",
    "# incluindo apenas features numericas, jogando fora os NaNs\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# passo adicional: criando features polinomiais\n",
    "# pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=1, include_bias=False)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)\n",
    "\n",
    "print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "# redefinindo as features de treino e de teste\n",
    "X_train = pf.transform(X_train)\n",
    "X_test = pf.transform(X_test)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"\\nMétricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:32.509954Z",
     "start_time": "2022-02-04T21:13:32.077934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "\n",
    "# incluindo apenas features numericas, jogando fora os NaNs\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# passo adicional: criando features polinomiais\n",
    "# pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)\n",
    "\n",
    "print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "# redefinindo as features de treino e de teste\n",
    "X_train = pf.transform(X_train)\n",
    "X_test = pf.transform(X_test)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"\\nMétricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esete ultimo modelo tinha muuuuuito mais parametros que observações, portanto, aprendeu perfeitamente até mesmo os ruidos da base de treino!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com quantas features o modelo final foi construído?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:32.525946Z",
     "start_time": "2022-02-04T21:13:32.513968Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.n_output_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa hipótese é:\n",
    "\n",
    "$$ f_{H, \\vec{b}}(\\vec{x}) = b_0 + b_1x_1 + b_2x_2 + \\cdots + b_{702} x_{702}$$\n",
    "\n",
    "Ou seja, temos um modelo **com muitos parâmetros**, ou seja, **muito complexo!**\n",
    "\n",
    "Com tantos parâmetros assim, há muitos **graus de liberdade** pra que a hipótese se ajuste até às particularidades da base de treino... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado é evidente: temos um modelo altamente **overfitado**, dado o número enorme de features após o transformer -- e isso porque estamos utilizando apenas features quadráticas, imagine se tivéssemos usado features de grau maior!\n",
    "\n",
    "É de se imaginar que muitas destas features não deveriam estar aí, não é mesmo?\n",
    "\n",
    "Oras, uma forma interessante de eliminar features é fazendo o que chamamos de **feature selection**.\n",
    "\n",
    "A ideia é a seguinte: gostaríamos sim de introduzir features quadráticas, aumentando um pouco a complexidade da hipótese, **mas não tanto!**. \n",
    "\n",
    "E é isso que conseguiremos fazer com as técnicas de **regularização**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes, vamos chutar mais o balde..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:32.890984Z",
     "start_time": "2022-02-04T21:13:32.529943Z"
    }
   },
   "outputs": [],
   "source": [
    "# uma saida, é limitar a transformação\n",
    "\n",
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "\n",
    "# incluindo apenas features numericas, jogando fora os NaNs\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# passo adicional: criando features polinomiais\n",
    "# pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)\n",
    "\n",
    "print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "# redefinindo as features de treino e de teste\n",
    "X_train = pf.transform(X_train)\n",
    "X_test = pf.transform(X_test)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"\\nMétricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:34.715801Z",
     "start_time": "2022-02-04T21:13:32.893984Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "\n",
    "# incluindo apenas features numericas, jogando fora os NaNs\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ======================================\n",
    "# passo adicional: criando features polinomiais\n",
    "# pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "# pra gente se acostumar: fit só em dados de treino!!!!\n",
    "pf.fit(X_train)\n",
    "\n",
    "print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "print(f\"Número de features no espaço transformado: {pf.n_output_features_}\\n\\n\")\n",
    "\n",
    "# redefinindo as features de treino e de teste\n",
    "X_train = pf.transform(X_train)\n",
    "X_test = pf.transform(X_test)\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression()\n",
    "\n",
    "# ======================================\n",
    "\n",
    "reg_lin.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Intercepto (b0): {reg_lin.intercept_}\")\n",
    "print(f\"Demais parâmetros (b1, ..., bn): {reg_lin.coef_}\")\n",
    "\n",
    "# ======================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# predições de treino\n",
    "y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "print(\"\\nMétricas de treino:\\n\")\n",
    "print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "# predições de teste\n",
    "y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "print(\"\\nMétricas de teste:\\n\")\n",
    "print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T01:14:14.508900Z",
     "start_time": "2022-01-28T01:14:14.485892Z"
    }
   },
   "source": [
    "O que podemos dizer sobre este modelo?\n",
    "\n",
    "**Claro overfitting!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra nosso código ficar mais orgamizado, podemos fazer uma função para a modelagem (depois vcs podem refazer os passos acima com a função, ajuda a organizar o código!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:34.747475Z",
     "start_time": "2022-02-04T21:13:34.715801Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "    \n",
    "def poly_reg(X, y, degree):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # ======================================\n",
    "    # passo adicional: criando features polinomiais\n",
    "    # pra deixar a hipotese mais complexa (regressão linear em espaço polinomial)\n",
    "\n",
    "    pf = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "\n",
    "    # pra gente se acostumar: fit só em dados de treino!!!!\n",
    "    pf.fit(X_train)\n",
    "\n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\")\n",
    "\n",
    "    # redefinindo as features de treino e de teste\n",
    "    X_train = pf.transform(X_train)\n",
    "    X_test = pf.transform(X_test)\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    reg_lin = LinearRegression()\n",
    "\n",
    "    reg_lin.fit(X_train, y_train)\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    # predições de treino\n",
    "    y_pred_train = reg_lin.predict(X_train)\n",
    "\n",
    "    print(\"\\nMétricas de treino:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "    # predições de teste\n",
    "    y_pred_test = reg_lin.predict(X_test)\n",
    "\n",
    "    print(\"\\nMétricas de teste:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:34.873400Z",
     "start_time": "2022-02-04T21:13:34.751471Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:34.985372Z",
     "start_time": "2022-02-04T21:13:34.876401Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_reg(X, y, degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:35.302135Z",
     "start_time": "2022-02-04T21:13:34.990366Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_reg(X, y, degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:36.553805Z",
     "start_time": "2022-02-04T21:13:35.308131Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_reg(X, y, degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## 2) Regularização\n",
    "\n",
    "Neste ponto, é muito importante que falemos sobre **regularização**.\n",
    "\n",
    "O objetivo da regularização é **diminuir a complexidade** de modelos, de modo a evitar que particularidades da base de treino (ruídos) sejam aprendidos (ou seja, evitar overfitting!)\n",
    "\n",
    "Uma outra forma de enxergar regularização: **diminuição do espaço de hipóteses!**\n",
    "\n",
    "<img src=https://curso-r.github.io/main-intro-ml/slides/static/img/erro_treino_erro_teste.png width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularização: problema de otimização VINCULADO!! ou seja, com restrições.\n",
    "\n",
    "problema de otimização: otimização da função de custo, que é o objetivo da aprendizagem, pra determinar o $\\hat{\\vec{b}}$\n",
    "\n",
    "restrições: é o que determina se temos L1 (lasso) ou L2 (ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão linear (sem regularização)\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/5408b0a7-85f3-4824-ad68-44867121ecb9.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 (Lasso)\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/acabe9da-07ba-4337-b467-dd2701a40cc8.png width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 (Ridge)\n",
    "\n",
    "<img src=https://s3-sa-east-1.amazonaws.com/lcpi/46eda310-fb2f-498b-b455-593183de1dd7.png width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber como relacionar $t$ com $\\lambda$, veja [este post](https://stats.stackexchange.com/questions/259177/expressing-the-lasso-regression-constraint-via-the-penalty-parameter) ou então [este](https://stats.stackexchange.com/questions/90648/kkt-versus-unconstrained-formulation-of-lasso-regression) -- discussões bem matemáticas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observações importantes:\n",
    "\n",
    "- $\\lambda$ é um parâmetro que controla a \"força\" da regularização<br><br>\n",
    "- **L1 pode zerar coeficientes** - faz feature selection<br><br>\n",
    "- **L2 apenas diminui o tamanho de coeficientes** - não faz feature selection<br><br>\n",
    "\n",
    "<img src=https://ugc.futurelearn.com/uploads/assets/2b/fe/2bfe399e-503e-4eae-9138-a3d7da738713.png width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Geometricamente:\n",
    "\n",
    "<img src=https://www.astroml.org/_images/fig_lasso_ridge_1.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No sklearn, é possível fazer um modelo de regressão linear regularizado facilmente com as classes respectivas:\n",
    "\n",
    "- [Regularização L2/Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\n",
    "\n",
    "- [Regularização L1/Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\n",
    "\n",
    "Há, no sklearn, também uma implementação para um tipo de regularização conhecida como **Elastic Net**:\n",
    "\n",
    "<img src=https://miro.medium.com/max/761/1*nrWncnoJ4V_BkzEf1pd4MA.png width=900>\n",
    "\n",
    "A classe se chama [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar regularização no dataset das casas, juntamente com as features polinomiais:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANTE**: como os métodos de regularização são baseados na norma do vetor de parâmetros, é muito importante que as features sejam escaladas para que os métodos funcionem bem!\n",
    "\n",
    "Isso porque a escala das features irá influenciar a regularização aplicada ao parâmetro respectivo!\n",
    "\n",
    "Para eliminar este efeito, escalar os dados é muito importante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar concretamente como a regularização de fato simplifica a hipótese! Pra isso, considere os pontos a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:36.569452Z",
     "start_time": "2022-02-04T21:13:36.553805Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ruido = np.random.normal(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:36.687852Z",
     "start_time": "2022-02-04T21:13:36.569452Z"
    }
   },
   "outputs": [],
   "source": [
    "ruido.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:36.796996Z",
     "start_time": "2022-02-04T21:13:36.687852Z"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.055153Z",
     "start_time": "2022-02-04T21:13:36.798197Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(10)\n",
    "y = X**2\n",
    "\n",
    "np.random.seed(42)\n",
    "ruido = np.random.normal(0, 3, 10)\n",
    "y = y + ruido\n",
    "\n",
    "# isso é só pra poder treinar o modelo com 1 feature\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "x_plot = np.linspace(0, 10, 1000)\n",
    "y_plot = x_plot**2\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.plot(x_plot, y_plot, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.195400Z",
     "start_time": "2022-02-04T21:13:37.055153Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.273520Z",
     "start_time": "2022-02-04T21:13:37.195400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.382961Z",
     "start_time": "2022-02-04T21:13:37.273520Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_y_plot(estimator, x_plot):\n",
    "\n",
    "    y_plot = estimator.intercept_\n",
    "\n",
    "    for n, b_n in enumerate(estimator.coef_):\n",
    "\n",
    "        y_plot = y_plot + b_n*(x_plot**(n+1))\n",
    "    \n",
    "    return y_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.490647Z",
     "start_time": "2022-02-04T21:13:37.382961Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.589982Z",
     "start_time": "2022-02-04T21:13:37.490647Z"
    }
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=False).fit(X)\n",
    "\n",
    "X_transf = pf.transform(X)\n",
    "\n",
    "X_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.695927Z",
     "start_time": "2022-02-04T21:13:37.593979Z"
    }
   },
   "outputs": [],
   "source": [
    "mms = MinMaxScaler().fit(X_transf)\n",
    "\n",
    "X_transf =  mms.transform(X_transf)\n",
    "\n",
    "X_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:37.823049Z",
     "start_time": "2022-02-04T21:13:37.695927Z"
    }
   },
   "outputs": [],
   "source": [
    "def reg_poly_plot(X, y, degree):\n",
    "    \n",
    "    # nestes caso APENAS, nao avaliaremos os modelos. Só queremos visualizar\n",
    "\n",
    "    pf = PolynomialFeatures(degree=degree, include_bias=False).fit(X)\n",
    "\n",
    "    X_transf = pf.transform(X)\n",
    "    \n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\")\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    mms = MinMaxScaler().fit(X_transf)\n",
    "\n",
    "    X_transf =  mms.transform(X_transf)\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    reg_lin = LinearRegression().fit(X_transf, y)\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    print(\"\\nModelo treinado:\")\n",
    "\n",
    "    x_plot = np.linspace(X_transf[:, 0].min(), X_transf[:, 0].max(), 1000)\n",
    "\n",
    "    y_plot = calc_y_plot(reg_lin, x_plot)\n",
    "\n",
    "    plt.scatter(X_transf[:, 0], y)\n",
    "\n",
    "    plt.plot(x_plot, y_plot, color=\"r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:39.434725Z",
     "start_time": "2022-02-04T21:13:37.823049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for degree in range(1, 11):\n",
    "    \n",
    "    reg_poly_plot(X, y, degree)\n",
    "    \n",
    "    print(\"#\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos regularizar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:39.450362Z",
     "start_time": "2022-02-04T21:13:39.434725Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "def reg_poly_regularized_plot(X, y, degree):\n",
    "    \n",
    "    # nestes caso APENAS, nao avaliaremos os modelos. Só queremos visualizar\n",
    "\n",
    "    pf = PolynomialFeatures(degree=degree, include_bias=False).fit(X)\n",
    "\n",
    "    X_transf = pf.transform(X)\n",
    "    \n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\")\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    mms = MinMaxScaler().fit(X_transf)\n",
    "\n",
    "    X_transf =  mms.transform(X_transf)\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    reg_lin = LinearRegression().fit(X_transf, y)\n",
    "    \n",
    "    reg_l1 = Lasso(alpha=1).fit(X_transf, y)\n",
    "    \n",
    "    reg_l2 = Ridge(alpha=1).fit(X_transf, y)\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    print(\"\\nModelo treinado:\")\n",
    "    \n",
    "    plt.scatter(X_transf[:, 0], y)\n",
    "\n",
    "    x_plot = np.linspace(X_transf[:, 0].min(), X_transf[:, 0].max(), 1000)\n",
    "\n",
    "    y_plot_reg_lin = calc_y_plot(reg_lin, x_plot)\n",
    "    plt.plot(x_plot, y_plot_reg_lin, color=\"r\", label=\"rl\", ls=\":\")\n",
    "    \n",
    "    y_plot_reg_l1 = calc_y_plot(reg_l1, x_plot)\n",
    "    plt.plot(x_plot, y_plot_reg_l1, color=\"orange\", label=\"L1\")\n",
    "    \n",
    "    y_plot_reg_l2 = calc_y_plot(reg_l2, x_plot)\n",
    "    plt.plot(x_plot, y_plot_reg_l2, color=\"green\", label=\"L2\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:39.549580Z",
     "start_time": "2022-02-04T21:13:39.455438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# pra ficar mais fácil de ver os parâmetros, vamos fixar três casas decimais\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{:.3f}\".format(x)})\n",
    "\n",
    "def reg_poly_regularized_plot2(X, y, degree):\n",
    "    \n",
    "    # nestes caso APENAS, nao avaliaremos os modelos. Só queremos visualizar\n",
    "\n",
    "    pf = PolynomialFeatures(degree=degree, include_bias=False).fit(X)\n",
    "\n",
    "    X_transf = pf.transform(X)\n",
    "    \n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\")\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    mms = MinMaxScaler().fit(X_transf)\n",
    "\n",
    "    X_transf =  mms.transform(X_transf)\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    reg_lin = LinearRegression().fit(X_transf, y)\n",
    "    \n",
    "    reg_l1 = Lasso(alpha=1).fit(X_transf, y)\n",
    "    \n",
    "    reg_l2 = Ridge(alpha=1).fit(X_transf, y)\n",
    "    \n",
    "    print(f\"\\nParâmetros modelo não regularizado:\\n{reg_lin.intercept_:.3f}\\n{reg_lin.coef_}\")\n",
    "    print(f\"\\nParâmetros modelo com L1 (Lasso):\\n{reg_l1.intercept_:.3f}\\n{reg_l1.coef_}\")\n",
    "    print(f\"\\nParâmetros modelo com L2 (Ridge):\\n{reg_l2.intercept_:.3f}\\n{reg_l2.coef_}\")\n",
    "\n",
    "    # =====================================\n",
    "\n",
    "    print(\"\\nModelo treinado:\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axs[0].scatter(X_transf[:, 0], y)\n",
    "    axs[1].scatter(X_transf[:, 0], y)\n",
    "\n",
    "    x_plot = np.linspace(X_transf[:, 0].min(), X_transf[:, 0].max(), 1000)\n",
    "\n",
    "    y_plot_reg_lin = calc_y_plot(reg_lin, x_plot)\n",
    "    axs[0].plot(x_plot, y_plot_reg_lin, color=\"r\", label=\"RL\", ls=\":\")\n",
    "    axs[1].plot(x_plot, y_plot_reg_lin, color=\"r\", label=\"RL\", ls=\":\")\n",
    "    \n",
    "    y_plot_reg_l1 = calc_y_plot(reg_l1, x_plot)\n",
    "    axs[0].plot(x_plot, y_plot_reg_l1, color=\"orange\", label=\"L1\")\n",
    "    \n",
    "    y_plot_reg_l2 = calc_y_plot(reg_l2, x_plot)\n",
    "    axs[1].plot(x_plot, y_plot_reg_l2, color=\"green\", label=\"L2\")\n",
    "    \n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:43.372871Z",
     "start_time": "2022-02-04T21:13:39.549580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for degree in range(1, 11):\n",
    "    \n",
    "    reg_poly_regularized_plot2(X, y, degree)\n",
    "    \n",
    "    print(\"#\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:13:45.348346Z",
     "start_time": "2022-02-04T21:13:43.377871Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for degree in range(1, 11):\n",
    "    \n",
    "    reg_poly_regularized_plot(X, y, degree)\n",
    "    \n",
    "    print(\"#\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lição de casa**: altere a função acima pra ter mais um argumento: alpha.\n",
    "\n",
    "Daí, varie também o alpha (força de regularização)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "Vamos agora voltar pro dataset de precificação de casas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "______________\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentário: discutimos já que é importante escalar as features quando formos usar regularização.\n",
    "\n",
    "Mas, é muito importante que o scaling dos dados seja O ÚLTIMO PASSO!\n",
    "\n",
    "Ou seja, se também quisermos fazer o polynomial features, temos que fazer ANTES do scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:35:14.754235Z",
     "start_time": "2022-02-04T22:35:14.738263Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:40:43.263986Z",
     "start_time": "2022-02-04T22:40:43.250991Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X)\n",
    "print()\n",
    "\n",
    "pf = PolynomialFeatures(degree=3, include_bias=False).fit(X)\n",
    "X_transf = pf.transform(X)\n",
    "\n",
    "print(X_transf)\n",
    "print()\n",
    "\n",
    "ss = StandardScaler().fit(X_transf)\n",
    "X_transf = ss.transform(X_transf)\n",
    "\n",
    "print(X_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:41:21.825216Z",
     "start_time": "2022-02-04T22:41:21.808209Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(X)\n",
    "print()\n",
    "\n",
    "ss = StandardScaler().fit(X)\n",
    "X_transf = ss.transform(X)\n",
    "\n",
    "print(X_transf)\n",
    "print()\n",
    "\n",
    "pf = PolynomialFeatures(degree=3, include_bias=False).fit(X_transf)\n",
    "X_transf = pf.transform(X_transf)\n",
    "\n",
    "print(X_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "______________\n",
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:25:58.684540Z",
     "start_time": "2022-02-04T22:25:58.668548Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:14:12.457271Z",
     "start_time": "2022-02-04T23:14:12.424288Z"
    }
   },
   "outputs": [],
   "source": [
    "def poly_regularized_reg(X, y, degree, \n",
    "                         type_regularization=None, alpha=1, l1_ratio=0.5, \n",
    "                         iter_max=1000):\n",
    "    '''\n",
    "    - type_regularization (str): opções de regularização: [\"l1\", \"l2\", \"en\", None]\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # ======================================\n",
    " \n",
    "    pf = PolynomialFeatures(degree=degree, include_bias=False).fit(X_train)\n",
    "\n",
    "    # redefinindo as features de treino e de teste\n",
    "    X_train = pf.transform(X_train)\n",
    "    X_test = pf.transform(X_test)\n",
    "    \n",
    "    print(f\"Número original de features: {pf.n_features_in_}\")\n",
    "    print(f\"Número de features no espaço transformado: {pf.n_output_features_}\")\n",
    "\n",
    "    # ======================================\n",
    "    # normalização dos dados - MUITO importante quando há regularização!!\n",
    "    # e é o passo imediatamente antes de treinar os modelos\n",
    "    \n",
    "    mms = MinMaxScaler().fit(X_train)\n",
    "    \n",
    "    X_train = mms.transform(X_train)\n",
    "    X_test = mms.transform(X_test)\n",
    "    \n",
    "    # ======================================\n",
    "\n",
    "    if type_regularization == \"l1\":\n",
    "        \n",
    "        model = Lasso(alpha=alpha, max_iter=iter_max).fit(X_train, y_train)\n",
    "        \n",
    "    elif type_regularization == \"l2\":\n",
    "        \n",
    "        model = Ridge(alpha=alpha, max_iter=iter_max).fit(X_train, y_train)\n",
    "        \n",
    "    elif type_regularization == \"en\":\n",
    "        \n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=iter_max).fit(X_train, y_train)\n",
    "        \n",
    "    elif type_regularization == None:\n",
    "    \n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        list_opcoes = [\"l1\", \"l2\", \"en\", None]\n",
    "        raise ValueError(f\"Opção de regularização indisponível!\\nOpções aceitas: {list_opcoes}\")\n",
    "\n",
    "\n",
    "    # ======================================\n",
    "\n",
    "    # predições de treino\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    print(\"\\nMétricas de treino:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "\n",
    "    # predições de teste\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"\\nMétricas de teste:\\n\")\n",
    "    print(f\"R^2: {r2_score(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:04:13.488332Z",
     "start_time": "2022-02-04T23:04:13.246466Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/house_prices.csv\")\n",
    "df = df.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:05:33.826623Z",
     "start_time": "2022-02-04T23:05:33.776651Z"
    }
   },
   "outputs": [],
   "source": [
    "# regressão linear, sem regularização\n",
    "# exatamente o que fizemos na primeira aula!\n",
    "poly_regularized_reg(X, y, degree=1, type_regularization=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:05:57.715041Z",
     "start_time": "2022-02-04T23:05:57.155363Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark** (modelo linear que conseguíamos fazer antes dessa aula)\n",
    "\n",
    "Métricas de treino:\n",
    "\n",
    "R^2: 0.81<br>\n",
    "MAE: 22286.55<br>\n",
    "RMSE: 35650.58<br>\n",
    "\n",
    "Métricas de teste:\n",
    "\n",
    "R^2: 0.80<br>\n",
    "MAE: 23662.02<br>\n",
    "RMSE: 39859.00<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:14:26.728975Z",
     "start_time": "2022-02-04T23:14:24.294180Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=\"l1\", alpha=100, iter_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:16:58.410641Z",
     "start_time": "2022-02-04T23:16:39.191762Z"
    }
   },
   "outputs": [],
   "source": [
    "## ESSE FOI O CAMPEÃO (por enquanto, rs)\n",
    "\n",
    "poly_regularized_reg(X, y, degree=3, type_regularization=\"l1\", alpha=100, iter_max=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:19:52.135405Z",
     "start_time": "2022-02-04T23:19:52.025470Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=\"l2\", alpha=200, iter_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:20:16.153973Z",
     "start_time": "2022-02-04T23:20:15.920749Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=\"l2\", alpha=50, iter_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:23:09.177524Z",
     "start_time": "2022-02-04T23:23:07.647885Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=3, type_regularization=\"l2\", alpha=50, iter_max=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:22:53.117228Z",
     "start_time": "2022-02-04T23:22:52.977310Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=\"en\", \n",
    "                     alpha=1, l1_ratio=0.5,\n",
    "                     iter_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:24:43.807025Z",
     "start_time": "2022-02-04T23:24:43.605140Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=2, type_regularization=\"en\", \n",
    "                     alpha=10, l1_ratio=0.8,\n",
    "                     iter_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T23:25:19.065178Z",
     "start_time": "2022-02-04T23:25:18.333596Z"
    }
   },
   "outputs": [],
   "source": [
    "poly_regularized_reg(X, y, degree=3, type_regularization=\"en\", \n",
    "                     alpha=100, l1_ratio=0.8,\n",
    "                     iter_max=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "Uma pergunta importante é: **como selecionar um valor adequado para os parâmetros de regularização?**\n",
    "\n",
    "Naturalmente, este é um hiperarâmetro bastante importante, dado que ele controla a \"força\" da regularização a ser aplicada.\n",
    "\n",
    "E, no caso do elastic net, o parâmetro de mistura também é muito relevante!\n",
    "\n",
    "Uma abordagem para a escolha de valores adequados de hiperparâmetros (processo chamado de **hyperparameter tuning**) é testar exaustivamente vários valores com o processo de **validação cruzada**, de modo a encontrarmos os melhores valores (e/ou combinação de valores) de hiperparâmetros.\n",
    "\n",
    "Antes de nos aprofundarmos no processo de tuning, vamos entender melhor o que é a validação cruzada!\n",
    "\n",
    "Para isso, veja o notebook da próxima aula! ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249.667px",
    "width": "359.667px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
